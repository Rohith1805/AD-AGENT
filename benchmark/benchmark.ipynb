{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c167c4",
   "metadata": {},
   "source": [
    "# # OpenAD Code-Generation Benchmark Notebook\n",
    "# This notebook benchmarks the OpenAD code-generation pipeline across multiple libraries (PyOD, PyGOD, Darts, sktime).\n",
    "# It measures success rate, total runtime, InfoMiner durations, and LLM token usage, then exports results.json and summary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb54927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: pandas in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: pygod in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pygod) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pygod) (3.4.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pygod) (1.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pygod) (65.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from scikit-learn->pygod) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from scikit-learn->pygod) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, types, json\n",
    "%pip install tiktoken faiss-cpu pandas matplotlib pygod\n",
    "# ensure project root is on path\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "55f3091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Setup Imports and Instrumentation\n",
    "# Install required packages (if needed) and import modules\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import your instrumentation and pipeline\n",
    "from benchmark.instrumentation import InstrumentedChatOpenAI, InstrumentedInfoMiner, InstrumentedCoder\n",
    "from main import compiled_full_graph, FullToolState\n",
    "\n",
    "import langchain_openai\n",
    "\n",
    "# Monkey-patch ChatOpenAI to our instrumented version\n",
    "langchain_openai.ChatOpenAI = InstrumentedChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a5727fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (3.11.16)\n",
      "Requirement already satisfied: fsspec in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (2025.3.2)\n",
      "Requirement already satisfied: jinja2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (6.1.1)\n",
      "Requirement already satisfied: pyparsing in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests->torch_geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests->torch_geometric) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch_geometric\n",
    "\n",
    "from pygod.utils import load_data\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.makedirs('pygod_data', exist_ok=True)\n",
    "for name in ['weibo']:\n",
    "    path = f'pygod_data/{name}.pt'\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading '{name}' dataset...\")\n",
    "        data = load_data(name)\n",
    "        torch.save(data, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a389d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 3. Define Experiment Configurations\n",
    "# Provide dataset paths for each library\n",
    "exp_configs = {\n",
    "    'pyod': {\n",
    "        'algorithm': ['ABOD','LOF','IForest'],\n",
    "        'dataset_train': './data/glass_train.mat',\n",
    "        'dataset_test': './data/glass_test.mat',\n",
    "        'parameters': {'contamination': 0.1}\n",
    "    },\n",
    "    'pygod': {\n",
    "        'algorithm': ['OCGNN','GCN','SCAN'],\n",
    "        'dataset_train': './pygod_data/graph1.pt',  # clone https://github.com/pygod-team/data\n",
    "        'dataset_test': './pygod_data/graph2.pt',\n",
    "        'parameters': {}\n",
    "    },\n",
    "    # 'darts': {\n",
    "    #     'algorithm': ['DifferenceScorer','NormScorer'],\n",
    "    #     'dataset_train': './data/yahoo_train.csv',\n",
    "    #     'dataset_test': './data/yahoo_test.csv',\n",
    "    #     'parameters': {}\n",
    "    # },\n",
    "    # 'sktime': {\n",
    "    #     'algorithm': ['KMeansScorer'],\n",
    "    #     'dataset_train': './data/yahoo_train.csv',\n",
    "    #     'dataset_test': './data/yahoo_test.csv',\n",
    "    #     'parameters': {}\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3e8b834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cache Hit] Using recent cache for MO-GAAL\n",
      "The `MO_GAAL` class in PyOD is designed for Multi-Objective Generative Adversarial Active Learning, which generates potential outliers to help classifiers effectively distinguish between normal data and outliers. To prevent mode collapse, it employs multiple generators with different objectives.\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `MO_GAAL` class with the following parameters:\n",
      "\n",
      "- **contamination**: float in (0., 0.5), optional (default=0.1)\n",
      "  - The proportion of outliers in the dataset. Used to define the threshold on the decision function.\n",
      "\n",
      "- **k**: int, optional (default=10)\n",
      "  - The number of sub-generators.\n",
      "\n",
      "- **stop_epochs**: int, optional (default=20)\n",
      "  - The number of training epochs. The total number of epochs equals three times this value.\n",
      "\n",
      "- **lr_d**: float, optional (default=0.01)\n",
      "  - Learning rate of the discriminator.\n",
      "\n",
      "- **lr_g**: float, optional (default=0.0001)\n",
      "  - Learning rate of the generator.\n",
      "\n",
      "- **momentum**: float, optional (default=0.9)\n",
      "  - Momentum parameter for SGD.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- **decision_scores_**: numpy array of shape (n_samples,)\n",
      "  - Outlier scores of the training data. Higher scores indicate more abnormal data points.\n",
      "\n",
      "- **threshold_**: float\n",
      "  - Threshold based on `contamination`, marking the most abnormal samples in `decision_scores_`.\n",
      "\n",
      "- **labels_**: int, either 0 or 1\n",
      "  - Binary labels of the training data, where 0 denotes inliers and 1 denotes outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"k\": 10,\n",
      "    \"stop_epochs\": 20,\n",
      "    \"lr_d\": 0.01,\n",
      "    \"lr_g\": 0.0001,\n",
      "    \"momentum\": 0.9\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary represents all parameters of the `__init__` method for the `MO_GAAL` class, along with their default values. \n",
      "[Cache Hit] Using recent cache for SO-GAAL\n",
      "The `SO-GAAL` (Single-Objective Generative Adversarial Active Learning) model in PyOD is designed to generate potential outliers to assist classifiers in effectively distinguishing outliers from normal data. To prevent mode collapse, the network structure can be expanded to multiple generators with different objectives, known as MO-GAAL. ([pyod.readthedocs.io](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/so_gaal.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method for the `SO_GAAL` class is defined as follows:\n",
      "\n",
      "\n",
      "```python\n",
      "def __init__(self, stop_epochs=20, lr_d=0.01, lr_g=0.0001, momentum=0.9, contamination=0.1):\n",
      "    super(SO_GAAL, self).__init__(contamination=contamination)\n",
      "    self.stop_epochs = stop_epochs\n",
      "    self.lr_d = lr_d\n",
      "    self.lr_g = lr_g\n",
      "    self.momentum = momentum\n",
      "```\n",
      "\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `stop_epochs` (int, optional, default=20): The number of epochs for training. The total number of epochs equals three times this value.\n",
      "- `lr_d` (float, optional, default=0.01): Learning rate for the discriminator.\n",
      "- `lr_g` (float, optional, default=0.0001): Learning rate for the generator.\n",
      "- `momentum` (float, optional, default=0.9): Momentum parameter for SGD.\n",
      "- `contamination` (float in (0., 0.5), optional, default=0.1): The proportion of outliers in the dataset. Used to define the threshold on the decision function.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_` (numpy array of shape (n_samples,)): Outlier scores of the training data. Higher scores indicate more abnormal instances.\n",
      "- `threshold_` (float): Threshold based on the `contamination` parameter, used to generate binary outlier labels.\n",
      "- `labels_` (int, either 0 or 1): Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"stop_epochs\": 20,\n",
      "    \"lr_d\": 0.01,\n",
      "    \"lr_g\": 0.0001,\n",
      "    \"momentum\": 0.9,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      " \n",
      "[Cache Hit] Using recent cache for AutoEncoder\n",
      "The `AutoEncoder` class in PyOD is a neural network model designed for unsupervised outlier detection by learning data representations and identifying anomalies through reconstruction errors. It shares similarities with Principal Component Analysis (PCA) in detecting outliers.\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `AutoEncoder` class is initialized with the following parameters:\n",
      "\n",
      "- **contamination**: `float` in (0., 0.5), optional (default=0.1)\n",
      "  - Proportion of outliers in the dataset, used to define the threshold on the decision function.\n",
      "\n",
      "- **preprocessing**: `bool`, optional (default=True)\n",
      "  - Indicates whether to apply preprocessing before training.\n",
      "\n",
      "- **lr**: `float`, optional (default=1e-3)\n",
      "  - Initial learning rate for the optimizer.\n",
      "\n",
      "- **epoch_num**: `int`, optional (default=10)\n",
      "  - Number of training epochs.\n",
      "\n",
      "- **batch_size**: `int`, optional (default=32)\n",
      "  - Batch size for training.\n",
      "\n",
      "- **optimizer_name**: `str`, optional (default='adam')\n",
      "  - Name of the optimizer used for training.\n",
      "\n",
      "- **device**: `str`, optional (default=None)\n",
      "  - Device to use for the model; if `None`, it is determined automatically.\n",
      "\n",
      "- **random_state**: `int`, optional (default=42)\n",
      "  - Random seed for reproducibility.\n",
      "\n",
      "- **use_compile**: `bool`, optional (default=False)\n",
      "  - Whether to compile the model; applicable for PyTorch version >= 2.0.0 and Python < 3.12.\n",
      "\n",
      "- **compile_mode**: `str`, optional (default='default')\n",
      "  - Mode to compile the model; options include “default”, “reduce-overhead”, “max-autotune”, or “max-autotune-no-cudagraphs”.\n",
      "\n",
      "- **verbose**: `int`, optional (default=1)\n",
      "  - Verbosity mode: 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "\n",
      "- **optimizer_params**: `dict`, optional (default={'weight_decay': 1e-5})\n",
      "  - Additional parameters for the optimizer, e.g., `{'weight_decay': 1e-5}`.\n",
      "\n",
      "- **hidden_neuron_list**: `list`, optional (default=[64, 32])\n",
      "  - Number of neurons per hidden layer; the network structure is [feature_size, 64, 32, 32, 64, feature_size].\n",
      "\n",
      "- **hidden_activation_name**: `str`, optional (default='relu')\n",
      "  - Activation function used in hidden layers.\n",
      "\n",
      "- **batch_norm**: `bool`, optional (default=True)\n",
      "  - Whether to apply Batch Normalization.\n",
      "\n",
      "- **dropout_rate**: `float` in (0., 1), optional (default=0.2)\n",
      "  - Dropout rate applied across all layers.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- **model**: `torch.nn.Module`\n",
      "  - The underlying AutoEncoder model.\n",
      "\n",
      "- **optimizer**: `torch.optim`\n",
      "  - Optimizer used to train the model.\n",
      "\n",
      "- **criterion**: `torch.nn.modules`\n",
      "  - Loss function used during training.\n",
      "\n",
      "- **decision_scores_**: `numpy array` of shape (n_samples,)\n",
      "  - Outlier scores of the training data; higher scores indicate more abnormal instances.\n",
      "\n",
      "- **threshold_**: `float`\n",
      "  - Threshold based on `contamination` to determine outliers.\n",
      "\n",
      "- **labels_**: `int`, either 0 or 1\n",
      "  - Binary labels of the training data; 0 for inliers and 1 for outliers.\n",
      "\n",
      "**Python Dictionary of `__init__` Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"preprocessing\": True,\n",
      "    \"lr\": 1e-3,\n",
      "    \"epoch_num\": 10,\n",
      "    \"batch_size\": 32,\n",
      "    \"optimizer_name\": \"adam\",\n",
      "    \"device\": None,\n",
      "    \"random_state\": 42,\n",
      "    \"use_compile\": False,\n",
      "    \"compile_mode\": \"default\",\n",
      "    \"verbose\": 1,\n",
      "    \"optimizer_params\": {\"weight_decay\": 1e-5},\n",
      "    \"hidden_neuron_list\": [64, 32],\n",
      "    \"hidden_activation_name\": \"relu\",\n",
      "    \"batch_norm\": True,\n",
      "    \"dropout_rate\": 0.2\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary provides a comprehensive overview of the initialization parameters and their default values for the `AutoEncoder` class in PyOD. \n",
      "[Cache Hit] Using recent cache for VAE\n",
      "The `VAE` (Variational Autoencoder) class in PyOD is designed for unsupervised outlier detection by reconstructing input data and evaluating reconstruction errors. It combines an encoder that maps input data to a latent space and a decoder that reconstructs the data from this latent representation. The model's loss function includes both reconstruction loss and Kullback-Leibler (KL) divergence, with an optional weighting parameter `beta` to emphasize the KL loss, facilitating the implementation of β-VAE.\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `VAE` class is initialized with the following parameters:\n",
      "\n",
      "- **contamination**: (float, default=0.1)\n",
      "  - Proportion of outliers in the dataset.\n",
      "- **preprocessing**: (bool, default=True)\n",
      "  - Whether to apply preprocessing before training.\n",
      "- **lr**: (float, default=1e-3)\n",
      "  - Initial learning rate for the optimizer.\n",
      "- **epoch_num**: (int, default=30)\n",
      "  - Number of training epochs.\n",
      "- **batch_size**: (int, default=32)\n",
      "  - Batch size for training.\n",
      "- **optimizer_name**: (str, default='adam')\n",
      "  - Name of the optimizer used for training.\n",
      "- **device**: (str, default=None)\n",
      "  - Device to use for computation (e.g., 'cpu', 'cuda').\n",
      "- **random_state**: (int, default=42)\n",
      "  - Random seed for reproducibility.\n",
      "- **use_compile**: (bool, default=False)\n",
      "  - Whether to compile the model before training.\n",
      "- **compile_mode**: (str, default='default')\n",
      "  - Compilation mode for the model.\n",
      "- **verbose**: (int, default=1)\n",
      "  - Verbosity level of training output.\n",
      "- **optimizer_params**: (dict, default={'weight_decay': 1e-5})\n",
      "  - Additional parameters for the optimizer.\n",
      "- **beta**: (float, default=1.0)\n",
      "  - Weight of the KL divergence term in the loss function.\n",
      "- **capacity**: (float, default=0.0)\n",
      "  - Maximum capacity of the loss bottleneck.\n",
      "- **encoder_neuron_list**: (list, default=[128, 64, 32])\n",
      "  - List specifying the number of neurons in each encoder layer.\n",
      "- **decoder_neuron_list**: (list, default=[32, 64, 128])\n",
      "  - List specifying the number of neurons in each decoder layer.\n",
      "- **latent_dim**: (int, default=2)\n",
      "  - Dimensionality of the latent space.\n",
      "- **hidden_activation_name**: (str, default='relu')\n",
      "  - Activation function for hidden layers.\n",
      "- **output_activation_name**: (str, default='sigmoid')\n",
      "  - Activation function for the output layer.\n",
      "- **batch_norm**: (bool, default=False)\n",
      "  - Whether to apply batch normalization.\n",
      "- **dropout_rate**: (float, default=0.2)\n",
      "  - Dropout rate for regularization.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- **decision_scores_**: (numpy array of shape (n_samples,))\n",
      "  - Outlier scores of the training data; higher scores indicate more abnormal instances.\n",
      "- **threshold_**: (float)\n",
      "  - Threshold based on the contamination parameter, used to generate binary outlier labels.\n",
      "- **labels_**: (numpy array of shape (n_samples,))\n",
      "  - Binary labels for the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Python Dictionary of `__init__` Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"preprocessing\": True,\n",
      "    \"lr\": 1e-3,\n",
      "    \"epoch_num\": 30,\n",
      "    \"batch_size\": 32,\n",
      "    \"optimizer_name\": \"adam\",\n",
      "    \"device\": None,\n",
      "    \"random_state\": 42,\n",
      "    \"use_compile\": False,\n",
      "    \"compile_mode\": \"default\",\n",
      "    \"verbose\": 1,\n",
      "    \"optimizer_params\": {\"weight_decay\": 1e-5},\n",
      "    \"beta\": 1.0,\n",
      "    \"capacity\": 0.0,\n",
      "    \"encoder_neuron_list\": [128, 64, 32],\n",
      "    \"decoder_neuron_list\": [32, 64, 128],\n",
      "    \"latent_dim\": 2,\n",
      "    \"hidden_activation_name\": \"relu\",\n",
      "    \"output_activation_name\": \"sigmoid\",\n",
      "    \"batch_norm\": False,\n",
      "    \"dropout_rate\": 0.2\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary provides a comprehensive overview of the initialization parameters and their default values for the `VAE` class in PyOD. \n",
      "[Cache Hit] Using recent cache for AnoGAN\n",
      "The `AnoGAN` class in PyOD is designed for anomaly detection using Generative Adversarial Networks (GANs). It is implemented in the `pyod.models.anogan` module. ([pyod.readthedocs.io](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `AnoGAN` class with several parameters that configure the model's architecture, training process, and other settings.\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `activation_hidden` (str, optional, default='tanh'): Activation function for hidden layers.\n",
      "- `dropout_rate` (float, optional, default=0.2): Dropout rate applied across all layers.\n",
      "- `latent_dim_G` (int, optional, default=2): Dimensionality of the generator's latent space.\n",
      "- `G_layers` (list, optional, default=[20, 10, 3, 10, 20]): Number of nodes per hidden layer in the generator.\n",
      "- `D_layers` (list, optional, default=[20, 10, 5]): Number of nodes per hidden layer in the discriminator.\n",
      "- `index_D_layer_for_recon_error` (int, optional, default=1): Index of the discriminator's hidden layer used for reconstruction error computation.\n",
      "- `epochs` (int, optional, default=500): Number of training epochs.\n",
      "- `preprocessing` (bool, optional, default=False): Whether to apply data standardization.\n",
      "- `learning_rate` (float, optional, default=0.001): Learning rate for training the network.\n",
      "- `learning_rate_query` (float, optional, default=0.01): Learning rate for backpropagation steps to approximate the query sample in the generator's latent space.\n",
      "- `epochs_query` (int, optional, default=20): Number of epochs for approximating the query sample in the generator's latent space.\n",
      "- `batch_size` (int, optional, default=32): Number of samples per gradient update.\n",
      "- `output_activation` (str, optional, default=None): Activation function for the output layer.\n",
      "- `contamination` (float, optional, default=0.1): Proportion of outliers in the dataset.\n",
      "- `device` (optional, default=None): Device to run the model on (e.g., 'cpu' or 'cuda').\n",
      "- `verbose` (int, optional, default=0): Verbosity mode (0 = silent, 1 = progress bar).\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_`: Numpy array of shape (n_samples,). Outlier scores of the training data.\n",
      "- `threshold_`: Float. Threshold based on `contamination` to generate binary outlier labels.\n",
      "- `labels_`: Int, either 0 or 1. Binary labels of the training data (0 for inliers, 1 for outliers).\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"activation_hidden\": \"tanh\",\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"latent_dim_G\": 2,\n",
      "    \"G_layers\": [20, 10, 3, 10, 20],\n",
      "    \"D_layers\": [20, 10, 5],\n",
      "    \"index_D_layer_for_recon_error\": 1,\n",
      "    \"epochs\": 500,\n",
      "    \"preprocessing\": False,\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"learning_rate_query\": 0.01,\n",
      "    \"epochs_query\": 20,\n",
      "    \"batch_size\": 32,\n",
      "    \"output_activation\": None,\n",
      "    \"contamination\": 0.1,\n",
      "    \"device\": None,\n",
      "    \"verbose\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary includes all parameters of the `__init__` method for the `AnoGAN` class, along with their default values. \n",
      "[Cache Hit] Using recent cache for DeepSVDD\n",
      "The `DeepSVDD` class in PyOD is designed for deep one-class classification, primarily used for anomaly detection. It trains a neural network to minimize the volume of a hypersphere that encloses the network representations of the data, effectively capturing the common factors of variation. This approach is detailed in the paper by Ruff et al. (2018).\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `__init__` method of the `DeepSVDD` class initializes the model with several parameters:\n",
      "\n",
      "- **n_features**: Number of features in the input data.\n",
      "- **c**: Deep SVDD center. If not provided, it is calculated based on the network's first forward pass.\n",
      "- **use_ae**: Boolean indicating whether to use the AutoEncoder type of DeepSVDD. Defaults to `False`.\n",
      "- **hidden_neurons**: List specifying the number of neurons per hidden layer. Defaults to `[64, 32]`.\n",
      "- **hidden_activation**: Activation function for hidden layers. Defaults to `'relu'`.\n",
      "- **output_activation**: Activation function for the output layer. Defaults to `'sigmoid'`.\n",
      "- **optimizer**: Optimizer for training. Defaults to `'adam'`.\n",
      "- **epochs**: Number of training epochs. Defaults to `100`.\n",
      "- **batch_size**: Number of samples per gradient update. Defaults to `32`.\n",
      "- **dropout_rate**: Dropout rate for layers. Defaults to `0.2`.\n",
      "- **l2_regularizer**: L2 regularization strength. Defaults to `0.1`.\n",
      "- **validation_size**: Proportion of data used for validation. Defaults to `0.1`.\n",
      "- **preprocessing**: Boolean indicating whether to standardize data. Defaults to `True`.\n",
      "- **verbose**: Verbosity mode. Defaults to `1`.\n",
      "- **random_state**: Seed for random number generation. Defaults to `None`.\n",
      "- **contamination**: Proportion of outliers in the data set. Defaults to `0.1`.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "After fitting the model, the following attributes are available:\n",
      "\n",
      "- **decision_scores_**: Anomaly scores of the training data. Higher scores indicate more abnormal instances.\n",
      "- **threshold_**: Threshold based on the contamination parameter, used to generate binary outlier labels.\n",
      "- **labels_**: Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Parameters Dictionary:**\n",
      "\n",
      "Here is a dictionary of all parameters for the `__init__` method, including their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"n_features\": None,\n",
      "    \"c\": None,\n",
      "    \"use_ae\": False,\n",
      "    \"hidden_neurons\": [64, 32],\n",
      "    \"hidden_activation\": \"relu\",\n",
      "    \"output_activation\": \"sigmoid\",\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"epochs\": 100,\n",
      "    \"batch_size\": 32,\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"l2_regularizer\": 0.1,\n",
      "    \"validation_size\": 0.1,\n",
      "    \"preprocessing\": True,\n",
      "    \"verbose\": 1,\n",
      "    \"random_state\": None,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Please note that `n_features` is a required parameter and does not have a default value. \n",
      "[Cache Hit] Using recent cache for ALAD\n",
      "The `ALAD` (Adversarially Learned Anomaly Detection) class in PyOD is designed for unsupervised anomaly detection using adversarial learning techniques. It implements the method described in the paper \"Adversarially Learned Anomaly Detection\" by Zenati et al. ([pyod.readthedocs.io](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `ALAD` class with several parameters that configure the model's architecture, training process, and other settings.\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `activation_hidden_gen` (str, optional, default='tanh'): Activation function for hidden layers in the generator (encoder and decoder) network.\n",
      "- `activation_hidden_disc` (str, optional, default='tanh'): Activation function for hidden layers in the discriminator networks.\n",
      "- `output_activation` (str, optional, default=None): Activation function for the output layers of the encoder and decoder.\n",
      "- `dropout_rate` (float, optional, default=0.2): Dropout rate applied across all layers.\n",
      "- `latent_dim` (int, optional, default=2): Dimensionality of the latent space.\n",
      "- `dec_layers` (list, optional, default=[5, 10, 25]): Number of nodes per hidden layer in the decoder network.\n",
      "- `enc_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the encoder network.\n",
      "- `disc_xx_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the discriminator network for input space.\n",
      "- `disc_zz_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the discriminator network for latent space.\n",
      "- `disc_xz_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the discriminator network for joint input and latent space.\n",
      "- `learning_rate_gen` (float, optional, default=0.0001): Learning rate for the generator network.\n",
      "- `learning_rate_disc` (float, optional, default=0.0001): Learning rate for the discriminator networks.\n",
      "- `add_recon_loss` (bool, optional, default=False): Whether to add reconstruction loss to the generator's loss function.\n",
      "- `lambda_recon_loss` (float, optional, default=0.1): Weight of the reconstruction loss in the generator's loss function.\n",
      "- `epochs` (int, optional, default=200): Number of training epochs.\n",
      "- `verbose` (int, optional, default=0): Verbosity mode.\n",
      "- `preprocessing` (bool, optional, default=False): Whether to apply data preprocessing (e.g., standardization).\n",
      "- `add_disc_zz_loss` (bool, optional, default=True): Whether to add loss from the latent space discriminator.\n",
      "- `spectral_normalization` (bool, optional, default=False): Whether to apply spectral normalization to the networks.\n",
      "- `batch_size` (int, optional, default=32): Number of samples per training batch.\n",
      "- `contamination` (float, optional, default=0.1): Proportion of outliers in the dataset.\n",
      "- `device` (torch.device, optional, default=None): Device to run the model on (e.g., 'cuda' or 'cpu').\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_` (numpy array of shape (n_samples,)): Outlier scores of the training data. Higher scores indicate more abnormal instances.\n",
      "- `threshold_` (float): Threshold based on the contamination parameter, used to generate binary outlier labels.\n",
      "- `labels_` (numpy array of shape (n_samples,)): Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"activation_hidden_gen\": \"tanh\",\n",
      "    \"activation_hidden_disc\": \"tanh\",\n",
      "    \"output_activation\": None,\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"latent_dim\": 2,\n",
      "    \"dec_layers\": [5, 10, 25],\n",
      "    \"enc_layers\": [25, 10, 5],\n",
      "    \"disc_xx_layers\": [25, 10, 5],\n",
      "    \"disc_zz_layers\": [25, 10, 5],\n",
      "    \"disc_xz_layers\": [25, 10, 5],\n",
      "    \"learning_rate_gen\": 0.0001,\n",
      "    \"learning_rate_disc\": 0.0001,\n",
      "    \"add_recon_loss\": False,\n",
      "    \"lambda_recon_loss\": 0.1,\n",
      "    \"epochs\": 200,\n",
      "    \"verbose\": 0,\n",
      "    \"preprocessing\": False,\n",
      "    \"add_disc_zz_loss\": True,\n",
      "    \"spectral_normalization\": False,\n",
      "    \"batch_size\": 32,\n",
      "    \"contamination\": 0.1,\n",
      "    \"device\": None\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary represents the parameters of the `ALAD` class's `__init__` method along with their default values. \n",
      "[Cache Hit] Using recent cache for AE1SVM\n",
      "The `AE1SVM` class in PyOD is an Autoencoder-based One-class Support Vector Machine designed for anomaly detection. It combines an autoencoder with a one-class SVM to effectively identify outliers in data.\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `AE1SVM` model with several parameters that control its architecture and training process.\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `hidden_neurons`: List of integers, optional (default=`[64, 32]`)\n",
      "  - Specifies the number of neurons in each hidden layer of the autoencoder.\n",
      "- `hidden_activation`: String, optional (default=`'relu'`)\n",
      "  - Defines the activation function for the hidden layers.\n",
      "- `batch_norm`: Boolean, optional (default=`True`)\n",
      "  - Indicates whether to apply batch normalization.\n",
      "- `learning_rate`: Float, optional (default=`1e-3`)\n",
      "  - Sets the learning rate for the optimizer.\n",
      "- `epochs`: Integer, optional (default=`50`)\n",
      "  - Determines the number of training epochs.\n",
      "- `batch_size`: Integer, optional (default=`32`)\n",
      "  - Specifies the size of each training batch.\n",
      "- `dropout_rate`: Float, optional (default=`0.2`)\n",
      "  - Sets the dropout rate for regularization.\n",
      "- `weight_decay`: Float, optional (default=`1e-5`)\n",
      "  - Defines the weight decay (L2 penalty) for the optimizer.\n",
      "- `preprocessing`: Boolean, optional (default=`True`)\n",
      "  - Indicates whether to apply standard scaling to the input data.\n",
      "- `loss_fn`: Callable, optional (default=`torch.nn.MSELoss()`)\n",
      "  - Specifies the loss function for reconstruction loss.\n",
      "- `contamination`: Float, optional (default=`0.1`)\n",
      "  - Represents the proportion of outliers in the data.\n",
      "- `alpha`: Float, optional (default=`1.0`)\n",
      "  - Weights the reconstruction loss in the final loss computation.\n",
      "- `sigma`: Float, optional (default=`1.0`)\n",
      "  - Sets the scaling factor for the random Fourier features.\n",
      "- `nu`: Float, optional (default=`0.1`)\n",
      "  - Parameter for the SVM loss.\n",
      "- `kernel_approx_features`: Integer, optional (default=`1000`)\n",
      "  - Number of random Fourier features to approximate the kernel.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_`:\n",
      "  - An array containing the outlier scores of the training data. Higher scores indicate a higher likelihood of being an outlier.\n",
      "- `threshold_`:\n",
      "  - The threshold value determined based on the `contamination` parameter, used to classify data points as inliers or outliers.\n",
      "- `labels_`:\n",
      "  - Binary labels for the training data, where 0 represents inliers and 1 represents outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"hidden_neurons\": [64, 32],\n",
      "    \"hidden_activation\": \"relu\",\n",
      "    \"batch_norm\": True,\n",
      "    \"learning_rate\": 1e-3,\n",
      "    \"epochs\": 50,\n",
      "    \"batch_size\": 32,\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"weight_decay\": 1e-5,\n",
      "    \"preprocessing\": True,\n",
      "    \"loss_fn\": \"torch.nn.MSELoss()\",\n",
      "    \"contamination\": 0.1,\n",
      "    \"alpha\": 1.0,\n",
      "    \"sigma\": 1.0,\n",
      "    \"nu\": 0.1,\n",
      "    \"kernel_approx_features\": 1000\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "*Note: The `loss_fn` parameter is represented as a string to ensure valid Python syntax for `ast.literal_eval`.*\n",
      "\n",
      "For more detailed information, you can refer to the official PyOD documentation:  \n",
      "[Cache Hit] Using recent cache for DevNet\n",
      "The `DevNet` class in PyOD is a deep learning-based anomaly detection model that utilizes deviation networks to identify outliers in data. It is implemented in the `pyod.models.devnet` module.\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method for the `DevNet` class is defined as follows:\n",
      "\n",
      "\n",
      "```python\n",
      "def __init__(self,\n",
      "             network_depth=2,\n",
      "             batch_size=512,\n",
      "             epochs=50,\n",
      "             nb_batch=20,\n",
      "             known_outliers=30,\n",
      "             cont_rate=0.02,\n",
      "             data_format=0,  # Assuming '0' for CSV\n",
      "             random_seed=42,\n",
      "             device=None,\n",
      "             contamination=0.1):\n",
      "    super(DevNet, self).__init__(contamination=contamination)\n",
      "    self._classes = 2\n",
      "    self.network_depth = network_depth\n",
      "    self.batch_size = batch_size\n",
      "    self.epochs = epochs\n",
      "    self.nb_batch = nb_batch\n",
      "    self.known_outliers = known_outliers\n",
      "    self.cont_rate = cont_rate\n",
      "    self.data_format = data_format\n",
      "    self.random_seed = random_seed\n",
      "    self.device = device\n",
      "    if self.device is None:\n",
      "        self.device = torch.device(\n",
      "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "```\n",
      "\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `network_depth` (default=2): Specifies the depth of the network.\n",
      "- `batch_size` (default=512): Defines the number of samples per batch during training.\n",
      "- `epochs` (default=50): Sets the number of training iterations over the entire dataset.\n",
      "- `nb_batch` (default=20): Indicates the number of batches per epoch.\n",
      "- `known_outliers` (default=30): Represents the number of known outliers used during training.\n",
      "- `cont_rate` (default=0.02): Denotes the contamination rate, i.e., the proportion of outliers in the dataset.\n",
      "- `data_format` (default=0): Assumes '0' for CSV format.\n",
      "- `random_seed` (default=42): Sets the seed for random number generation to ensure reproducibility.\n",
      "- `device` (default=None): Specifies the device for computation (e.g., 'cuda' or 'cpu'). If `None`, it defaults to 'cuda' if available; otherwise, 'cpu'.\n",
      "- `contamination` (default=0.1): Indicates the expected proportion of outliers in the dataset.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `self._classes`: Set to 2, indicating binary classification (inliers vs. outliers).\n",
      "- `self.network_depth`: Stores the specified network depth.\n",
      "- `self.batch_size`: Stores the batch size for training.\n",
      "- `self.epochs`: Stores the number of training epochs.\n",
      "- `self.nb_batch`: Stores the number of batches per epoch.\n",
      "- `self.known_outliers`: Stores the number of known outliers.\n",
      "- `self.cont_rate`: Stores the contamination rate.\n",
      "- `self.data_format`: Stores the data format.\n",
      "- `self.random_seed`: Stores the random seed value.\n",
      "- `self.device`: Stores the computation device.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"network_depth\": 2,\n",
      "    \"batch_size\": 512,\n",
      "    \"epochs\": 50,\n",
      "    \"nb_batch\": 20,\n",
      "    \"known_outliers\": 30,\n",
      "    \"cont_rate\": 0.02,\n",
      "    \"data_format\": 0,\n",
      "    \"random_seed\": 42,\n",
      "    \"device\": None,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary represents all parameters of the `__init__` method for the `DevNet` class, along with their default values. \n",
      "[Cache Hit] Using recent cache for LUNAR\n",
      "The `LUNAR` class in PyOD is designed for outlier detection by leveraging Graph Neural Networks to unify local outlier detection methods. It offers two model types: `SCORE_MODEL`, which directly outputs anomaly scores, and `WEIGHT_MODEL`, which outputs weights for k-nearest neighbor distances to compute anomaly scores. The class includes parameters for model configuration, negative sample generation, training settings, and data normalization.\n",
      "\n",
      "The `__init__` method of the `LUNAR` class has the following parameters with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"model_type\": \"WEIGHT\",\n",
      "    \"n_neighbours\": 5,\n",
      "    \"negative_sampling\": \"MIXED\",\n",
      "    \"val_size\": 0.1,\n",
      "    \"scaler\": \"MinMaxScaler()\",\n",
      "    \"epsilon\": 0.1,\n",
      "    \"proportion\": 1.0,\n",
      "    \"n_epochs\": 200,\n",
      "    \"lr\": 0.001,\n",
      "    \"wd\": 0.1,\n",
      "    \"verbose\": 0,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Note: The `scaler` parameter's default value is an instance of `MinMaxScaler()`, which is represented as a string to ensure valid Python syntax for `ast.literal_eval`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>infominer_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MO-GAAL</td>\n",
       "      <td>0.004609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SO-GAAL</td>\n",
       "      <td>0.000713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AutoEncoder</td>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VAE</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AnoGAN</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DeepSVDD</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ALAD</td>\n",
       "      <td>0.000404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AE1SVM</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DevNet</td>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LUNAR</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     algorithm  infominer_time\n",
       "0      MO-GAAL        0.004609\n",
       "1      SO-GAAL        0.000713\n",
       "2  AutoEncoder        0.000570\n",
       "3          VAE        0.000506\n",
       "4       AnoGAN        0.000483\n",
       "5     DeepSVDD        0.000439\n",
       "6         ALAD        0.000404\n",
       "7       AE1SVM        0.000387\n",
       "8       DevNet        0.000334\n",
       "9        LUNAR        0.000349"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mean    0.000879\n",
       "std     0.001315\n",
       "Name: infominer_time, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 2. Helper Function for PyOD InfoMiner\n",
    "\n",
    "def run_pyod_infominer(algorithms, train_path, test_path, params):\n",
    "    # Directly benchmark InfoMiner.query_docs without running the full pipeline\n",
    "    infom = InstrumentedInfoMiner()\n",
    "    results = []\n",
    "    for algo in algorithms:\n",
    "        # Time a single documentation query\n",
    "        _ = infom.query_docs(algo, None, 'pyod')\n",
    "        results.append({\n",
    "            'algorithm': algo,\n",
    "            'infominer_time': infom.last_query_duration\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Run Benchmark for Selected PyOD Algorithms\n",
    "algos = [\n",
    "    'MO-GAAL','SO-GAAL','AutoEncoder','VAE','AnoGAN',\n",
    "    'DeepSVDD','ALAD','AE1SVM','DevNet','LUNAR'\n",
    "]\n",
    "train_file = './data/glass_train.mat'\n",
    "test_file  = './data/glass_test.mat'\n",
    "params = {'contamination': 0.1}\n",
    "\n",
    "metrics = run_pyod_infominer(algos, train_file, test_file, params)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metrics)\n",
    "df.to_json('pyod_infominer_times.json', orient='records', indent=2)\n",
    "\n",
    "display(df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Summary of InfoMiner Time\n",
    "summary = df['infominer_time'].agg(['mean','std'])\n",
    "display(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
