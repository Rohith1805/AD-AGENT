{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c167c4",
   "metadata": {},
   "source": [
    "# # OpenAD Code-Generation Benchmark Notebook\n",
    "# This notebook benchmarks the OpenAD code-generation pipeline across multiple libraries (PyOD, PyGOD, Darts, sktime).\n",
    "# It measures success rate, total runtime, InfoMiner durations, and LLM token usage, then exports results.json and summary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb54927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: pandas in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: pygod in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pygod) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pygod) (3.4.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pygod) (1.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from pygod) (65.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from scikit-learn->pygod) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from scikit-learn->pygod) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, types, json\n",
    "%pip install tiktoken faiss-cpu pandas matplotlib pygod\n",
    "# ensure project root is on path\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "55f3091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Setup Imports and Instrumentation\n",
    "# Install required packages (if needed) and import modules\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import your instrumentation and pipeline\n",
    "from benchmark.instrumentation import InstrumentedChatOpenAI, InstrumentedInfoMiner, InstrumentedCoder\n",
    "from main import compiled_full_graph, FullToolState\n",
    "\n",
    "import langchain_openai\n",
    "\n",
    "# Monkey-patch ChatOpenAI to our instrumented version\n",
    "langchain_openai.ChatOpenAI = InstrumentedChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64a81272",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "def run_library_benchmark(lib_name, experiment_config):\n",
    "    \"\"\"\n",
    "    Runs the OpenAD pipeline up to code generation for a single library.\n",
    "    Returns per-algorithm metrics.\n",
    "    \"\"\"\n",
    "    # Prepare initial state\n",
    "    base_state: FullToolState = {\n",
    "        \"messages\": [],\n",
    "        \"current_tool\": \"\",\n",
    "        \"input_parameters\": {},\n",
    "        \"data_path_train\": experiment_config['dataset_train'],\n",
    "        \"data_path_test\": experiment_config['dataset_test'],\n",
    "        \"package_name\": lib_name,\n",
    "        \"agent_infominer\": InstrumentedInfoMiner(),\n",
    "        \"agent_coder\": InstrumentedCoder(),\n",
    "        \"agent_reviewer\": None,\n",
    "        \"agent_evaluator\": None,\n",
    "        \"agent_optimizer\": None,\n",
    "        \"vectorstore\": None,\n",
    "        \"code_quality\": None,\n",
    "        \"should_rerun\": False,\n",
    "        \"agent_preprocessor\": None,\n",
    "        \"agent_selector\": None,\n",
    "        \"experiment_config\": experiment_config,\n",
    "        \"results\": None,\n",
    "        \"algorithm_doc\": None,\n",
    "    }\n",
    "    \n",
    "    # Invoke only the process_all_tools node to skip preprocessor/selector\n",
    "    from main import process_all_tools\n",
    "    final_state = compiled_full_graph.invoke(base_state, config={\"recursion_limit\": 20})\n",
    "    # Extract metrics\n",
    "    metrics = []\n",
    "    for tool, tstate in final_state['results']:\n",
    "        coder = base_state['agent_coder']\n",
    "        infom = base_state['agent_infominer']\n",
    "        # success if code generated and no exception\n",
    "        success = hasattr(tstate.get('code_quality'), 'code')\n",
    "        gen_time = coder.last_generation_duration\n",
    "        info_time = infom.last_query_duration\n",
    "        tokens_in = langchain_openai.ChatOpenAI().input_tokens\n",
    "        tokens_out = langchain_openai.ChatOpenAI().output_tokens\n",
    "        metrics.append({\n",
    "            'library': lib_name,\n",
    "            'algorithm': tool,\n",
    "            'success': success,\n",
    "            'generation_time': gen_time,\n",
    "            'infominer_time': info_time,\n",
    "            'tokens_in': tokens_in,\n",
    "            'tokens_out': tokens_out,\n",
    "        })\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a5727fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (3.11.16)\n",
      "Requirement already satisfied: fsspec in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (2025.3.2)\n",
      "Requirement already satisfied: jinja2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (6.1.1)\n",
      "Requirement already satisfied: pyparsing in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests->torch_geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michaelx/OpenAD/.venv/lib/python3.11/site-packages (from requests->torch_geometric) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch_geometric\n",
    "\n",
    "from pygod.utils import load_data\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.makedirs('pygod_data', exist_ok=True)\n",
    "for name in ['weibo']:\n",
    "    path = f'pygod_data/{name}.pt'\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading '{name}' dataset...\")\n",
    "        data = load_data(name)\n",
    "        torch.save(data, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a389d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 3. Define Experiment Configurations\n",
    "# Provide dataset paths for each library\n",
    "exp_configs = {\n",
    "    'pyod': {\n",
    "        'algorithm': ['ABOD','LOF','IForest'],\n",
    "        'dataset_train': './data/glass_train.mat',\n",
    "        'dataset_test': './data/glass_test.mat',\n",
    "        'parameters': {'contamination': 0.1}\n",
    "    },\n",
    "    'pygod': {\n",
    "        'algorithm': ['OCGNN','GCN','SCAN'],\n",
    "        'dataset_train': './pygod_data/graph1.pt',  # clone https://github.com/pygod-team/data\n",
    "        'dataset_test': './pygod_data/graph2.pt',\n",
    "        'parameters': {}\n",
    "    },\n",
    "    # 'darts': {\n",
    "    #     'algorithm': ['DifferenceScorer','NormScorer'],\n",
    "    #     'dataset_train': './data/yahoo_train.csv',\n",
    "    #     'dataset_test': './data/yahoo_test.csv',\n",
    "    #     'parameters': {}\n",
    "    # },\n",
    "    # 'sktime': {\n",
    "    #     'algorithm': ['KMeansScorer'],\n",
    "    #     'dataset_train': './data/yahoo_train.csv',\n",
    "    #     'dataset_test': './data/yahoo_test.csv',\n",
    "    #     'parameters': {}\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3e8b834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cache Hit] Using recent cache for MO-GAAL\n",
      "The `MO_GAAL` class in PyOD is designed for Multi-Objective Generative Adversarial Active Learning, which generates potential outliers to help classifiers effectively distinguish between normal data and outliers. To prevent mode collapse, it employs multiple generators with different objectives.\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `MO_GAAL` class with the following parameters:\n",
      "\n",
      "- **contamination**: float in (0., 0.5), optional (default=0.1)\n",
      "  - The proportion of outliers in the dataset. Used to define the threshold on the decision function.\n",
      "\n",
      "- **k**: int, optional (default=10)\n",
      "  - The number of sub-generators.\n",
      "\n",
      "- **stop_epochs**: int, optional (default=20)\n",
      "  - The number of training epochs. The total number of epochs equals three times this value.\n",
      "\n",
      "- **lr_d**: float, optional (default=0.01)\n",
      "  - Learning rate of the discriminator.\n",
      "\n",
      "- **lr_g**: float, optional (default=0.0001)\n",
      "  - Learning rate of the generator.\n",
      "\n",
      "- **momentum**: float, optional (default=0.9)\n",
      "  - Momentum parameter for SGD.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- **decision_scores_**: numpy array of shape (n_samples,)\n",
      "  - Outlier scores of the training data. Higher scores indicate more abnormal data points.\n",
      "\n",
      "- **threshold_**: float\n",
      "  - Threshold based on `contamination`, marking the most abnormal samples in `decision_scores_`.\n",
      "\n",
      "- **labels_**: int, either 0 or 1\n",
      "  - Binary labels of the training data, where 0 denotes inliers and 1 denotes outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"k\": 10,\n",
      "    \"stop_epochs\": 20,\n",
      "    \"lr_d\": 0.01,\n",
      "    \"lr_g\": 0.0001,\n",
      "    \"momentum\": 0.9\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary represents all parameters of the `__init__` method for the `MO_GAAL` class, along with their default values. \n",
      "[Cache Hit] Using recent cache for SO-GAAL\n",
      "The `SO-GAAL` (Single-Objective Generative Adversarial Active Learning) model in PyOD is designed to generate potential outliers to assist classifiers in effectively distinguishing outliers from normal data. To prevent mode collapse, the network structure can be expanded to multiple generators with different objectives, known as MO-GAAL. ([pyod.readthedocs.io](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/so_gaal.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method for the `SO_GAAL` class is defined as follows:\n",
      "\n",
      "\n",
      "```python\n",
      "def __init__(self, stop_epochs=20, lr_d=0.01, lr_g=0.0001, momentum=0.9, contamination=0.1):\n",
      "    super(SO_GAAL, self).__init__(contamination=contamination)\n",
      "    self.stop_epochs = stop_epochs\n",
      "    self.lr_d = lr_d\n",
      "    self.lr_g = lr_g\n",
      "    self.momentum = momentum\n",
      "```\n",
      "\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `stop_epochs` (int, optional, default=20): The number of epochs for training. The total number of epochs equals three times this value.\n",
      "- `lr_d` (float, optional, default=0.01): Learning rate for the discriminator.\n",
      "- `lr_g` (float, optional, default=0.0001): Learning rate for the generator.\n",
      "- `momentum` (float, optional, default=0.9): Momentum parameter for SGD.\n",
      "- `contamination` (float in (0., 0.5), optional, default=0.1): The proportion of outliers in the dataset. Used to define the threshold on the decision function.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_` (numpy array of shape (n_samples,)): Outlier scores of the training data. Higher scores indicate more abnormal instances.\n",
      "- `threshold_` (float): Threshold based on the `contamination` parameter, used to generate binary outlier labels.\n",
      "- `labels_` (int, either 0 or 1): Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"stop_epochs\": 20,\n",
      "    \"lr_d\": 0.01,\n",
      "    \"lr_g\": 0.0001,\n",
      "    \"momentum\": 0.9,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      " \n",
      "[Cache Hit] Using recent cache for AutoEncoder\n",
      "The `AutoEncoder` class in PyOD is a neural network model designed for unsupervised outlier detection by learning data representations and identifying anomalies through reconstruction errors. It shares similarities with Principal Component Analysis (PCA) in detecting outliers.\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `AutoEncoder` class is initialized with the following parameters:\n",
      "\n",
      "- **contamination**: `float` in (0., 0.5), optional (default=0.1)\n",
      "  - Proportion of outliers in the dataset, used to define the threshold on the decision function.\n",
      "\n",
      "- **preprocessing**: `bool`, optional (default=True)\n",
      "  - Indicates whether to apply preprocessing before training.\n",
      "\n",
      "- **lr**: `float`, optional (default=1e-3)\n",
      "  - Initial learning rate for the optimizer.\n",
      "\n",
      "- **epoch_num**: `int`, optional (default=10)\n",
      "  - Number of training epochs.\n",
      "\n",
      "- **batch_size**: `int`, optional (default=32)\n",
      "  - Batch size for training.\n",
      "\n",
      "- **optimizer_name**: `str`, optional (default='adam')\n",
      "  - Name of the optimizer used for training.\n",
      "\n",
      "- **device**: `str`, optional (default=None)\n",
      "  - Device to use for the model; if `None`, it is determined automatically.\n",
      "\n",
      "- **random_state**: `int`, optional (default=42)\n",
      "  - Random seed for reproducibility.\n",
      "\n",
      "- **use_compile**: `bool`, optional (default=False)\n",
      "  - Whether to compile the model; applicable for PyTorch version >= 2.0.0 and Python < 3.12.\n",
      "\n",
      "- **compile_mode**: `str`, optional (default='default')\n",
      "  - Mode to compile the model; options include “default”, “reduce-overhead”, “max-autotune”, or “max-autotune-no-cudagraphs”.\n",
      "\n",
      "- **verbose**: `int`, optional (default=1)\n",
      "  - Verbosity mode: 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "\n",
      "- **optimizer_params**: `dict`, optional (default={'weight_decay': 1e-5})\n",
      "  - Additional parameters for the optimizer, e.g., `{'weight_decay': 1e-5}`.\n",
      "\n",
      "- **hidden_neuron_list**: `list`, optional (default=[64, 32])\n",
      "  - Number of neurons per hidden layer; the network structure is [feature_size, 64, 32, 32, 64, feature_size].\n",
      "\n",
      "- **hidden_activation_name**: `str`, optional (default='relu')\n",
      "  - Activation function used in hidden layers.\n",
      "\n",
      "- **batch_norm**: `bool`, optional (default=True)\n",
      "  - Whether to apply Batch Normalization.\n",
      "\n",
      "- **dropout_rate**: `float` in (0., 1), optional (default=0.2)\n",
      "  - Dropout rate applied across all layers.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- **model**: `torch.nn.Module`\n",
      "  - The underlying AutoEncoder model.\n",
      "\n",
      "- **optimizer**: `torch.optim`\n",
      "  - Optimizer used to train the model.\n",
      "\n",
      "- **criterion**: `torch.nn.modules`\n",
      "  - Loss function used during training.\n",
      "\n",
      "- **decision_scores_**: `numpy array` of shape (n_samples,)\n",
      "  - Outlier scores of the training data; higher scores indicate more abnormal instances.\n",
      "\n",
      "- **threshold_**: `float`\n",
      "  - Threshold based on `contamination` to determine outliers.\n",
      "\n",
      "- **labels_**: `int`, either 0 or 1\n",
      "  - Binary labels of the training data; 0 for inliers and 1 for outliers.\n",
      "\n",
      "**Python Dictionary of `__init__` Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"preprocessing\": True,\n",
      "    \"lr\": 1e-3,\n",
      "    \"epoch_num\": 10,\n",
      "    \"batch_size\": 32,\n",
      "    \"optimizer_name\": \"adam\",\n",
      "    \"device\": None,\n",
      "    \"random_state\": 42,\n",
      "    \"use_compile\": False,\n",
      "    \"compile_mode\": \"default\",\n",
      "    \"verbose\": 1,\n",
      "    \"optimizer_params\": {\"weight_decay\": 1e-5},\n",
      "    \"hidden_neuron_list\": [64, 32],\n",
      "    \"hidden_activation_name\": \"relu\",\n",
      "    \"batch_norm\": True,\n",
      "    \"dropout_rate\": 0.2\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary provides a comprehensive overview of the initialization parameters and their default values for the `AutoEncoder` class in PyOD. \n",
      "[Cache Hit] Using recent cache for VAE\n",
      "The `VAE` (Variational Autoencoder) class in PyOD is designed for unsupervised outlier detection by reconstructing input data and evaluating reconstruction errors. It combines an encoder that maps input data to a latent space and a decoder that reconstructs the data from this latent representation. The model's loss function includes both reconstruction loss and Kullback-Leibler (KL) divergence, with an optional weighting parameter `beta` to emphasize the KL loss, facilitating the implementation of β-VAE.\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `VAE` class is initialized with the following parameters:\n",
      "\n",
      "- **contamination**: (float, default=0.1)\n",
      "  - Proportion of outliers in the dataset.\n",
      "- **preprocessing**: (bool, default=True)\n",
      "  - Whether to apply preprocessing before training.\n",
      "- **lr**: (float, default=1e-3)\n",
      "  - Initial learning rate for the optimizer.\n",
      "- **epoch_num**: (int, default=30)\n",
      "  - Number of training epochs.\n",
      "- **batch_size**: (int, default=32)\n",
      "  - Batch size for training.\n",
      "- **optimizer_name**: (str, default='adam')\n",
      "  - Name of the optimizer used for training.\n",
      "- **device**: (str, default=None)\n",
      "  - Device to use for computation (e.g., 'cpu', 'cuda').\n",
      "- **random_state**: (int, default=42)\n",
      "  - Random seed for reproducibility.\n",
      "- **use_compile**: (bool, default=False)\n",
      "  - Whether to compile the model before training.\n",
      "- **compile_mode**: (str, default='default')\n",
      "  - Compilation mode for the model.\n",
      "- **verbose**: (int, default=1)\n",
      "  - Verbosity level of training output.\n",
      "- **optimizer_params**: (dict, default={'weight_decay': 1e-5})\n",
      "  - Additional parameters for the optimizer.\n",
      "- **beta**: (float, default=1.0)\n",
      "  - Weight of the KL divergence term in the loss function.\n",
      "- **capacity**: (float, default=0.0)\n",
      "  - Maximum capacity of the loss bottleneck.\n",
      "- **encoder_neuron_list**: (list, default=[128, 64, 32])\n",
      "  - List specifying the number of neurons in each encoder layer.\n",
      "- **decoder_neuron_list**: (list, default=[32, 64, 128])\n",
      "  - List specifying the number of neurons in each decoder layer.\n",
      "- **latent_dim**: (int, default=2)\n",
      "  - Dimensionality of the latent space.\n",
      "- **hidden_activation_name**: (str, default='relu')\n",
      "  - Activation function for hidden layers.\n",
      "- **output_activation_name**: (str, default='sigmoid')\n",
      "  - Activation function for the output layer.\n",
      "- **batch_norm**: (bool, default=False)\n",
      "  - Whether to apply batch normalization.\n",
      "- **dropout_rate**: (float, default=0.2)\n",
      "  - Dropout rate for regularization.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- **decision_scores_**: (numpy array of shape (n_samples,))\n",
      "  - Outlier scores of the training data; higher scores indicate more abnormal instances.\n",
      "- **threshold_**: (float)\n",
      "  - Threshold based on the contamination parameter, used to generate binary outlier labels.\n",
      "- **labels_**: (numpy array of shape (n_samples,))\n",
      "  - Binary labels for the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Python Dictionary of `__init__` Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"preprocessing\": True,\n",
      "    \"lr\": 1e-3,\n",
      "    \"epoch_num\": 30,\n",
      "    \"batch_size\": 32,\n",
      "    \"optimizer_name\": \"adam\",\n",
      "    \"device\": None,\n",
      "    \"random_state\": 42,\n",
      "    \"use_compile\": False,\n",
      "    \"compile_mode\": \"default\",\n",
      "    \"verbose\": 1,\n",
      "    \"optimizer_params\": {\"weight_decay\": 1e-5},\n",
      "    \"beta\": 1.0,\n",
      "    \"capacity\": 0.0,\n",
      "    \"encoder_neuron_list\": [128, 64, 32],\n",
      "    \"decoder_neuron_list\": [32, 64, 128],\n",
      "    \"latent_dim\": 2,\n",
      "    \"hidden_activation_name\": \"relu\",\n",
      "    \"output_activation_name\": \"sigmoid\",\n",
      "    \"batch_norm\": False,\n",
      "    \"dropout_rate\": 0.2\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary provides a comprehensive overview of the initialization parameters and their default values for the `VAE` class in PyOD. \n",
      "[Cache Hit] Using recent cache for AnoGAN\n",
      "The `AnoGAN` class in PyOD is designed for anomaly detection using Generative Adversarial Networks (GANs). It is implemented in the `pyod.models.anogan` module. ([pyod.readthedocs.io](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `AnoGAN` class with several parameters that configure the model's architecture, training process, and other settings.\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `activation_hidden` (str, optional, default='tanh'): Activation function for hidden layers.\n",
      "- `dropout_rate` (float, optional, default=0.2): Dropout rate applied across all layers.\n",
      "- `latent_dim_G` (int, optional, default=2): Dimensionality of the generator's latent space.\n",
      "- `G_layers` (list, optional, default=[20, 10, 3, 10, 20]): Number of nodes per hidden layer in the generator.\n",
      "- `D_layers` (list, optional, default=[20, 10, 5]): Number of nodes per hidden layer in the discriminator.\n",
      "- `index_D_layer_for_recon_error` (int, optional, default=1): Index of the discriminator's hidden layer used for reconstruction error computation.\n",
      "- `epochs` (int, optional, default=500): Number of training epochs.\n",
      "- `preprocessing` (bool, optional, default=False): Whether to apply data standardization.\n",
      "- `learning_rate` (float, optional, default=0.001): Learning rate for training the network.\n",
      "- `learning_rate_query` (float, optional, default=0.01): Learning rate for backpropagation steps to approximate the query sample in the generator's latent space.\n",
      "- `epochs_query` (int, optional, default=20): Number of epochs for approximating the query sample in the generator's latent space.\n",
      "- `batch_size` (int, optional, default=32): Number of samples per gradient update.\n",
      "- `output_activation` (str, optional, default=None): Activation function for the output layer.\n",
      "- `contamination` (float, optional, default=0.1): Proportion of outliers in the dataset.\n",
      "- `device` (optional, default=None): Device to run the model on (e.g., 'cpu' or 'cuda').\n",
      "- `verbose` (int, optional, default=0): Verbosity mode (0 = silent, 1 = progress bar).\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_`: Numpy array of shape (n_samples,). Outlier scores of the training data.\n",
      "- `threshold_`: Float. Threshold based on `contamination` to generate binary outlier labels.\n",
      "- `labels_`: Int, either 0 or 1. Binary labels of the training data (0 for inliers, 1 for outliers).\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"activation_hidden\": \"tanh\",\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"latent_dim_G\": 2,\n",
      "    \"G_layers\": [20, 10, 3, 10, 20],\n",
      "    \"D_layers\": [20, 10, 5],\n",
      "    \"index_D_layer_for_recon_error\": 1,\n",
      "    \"epochs\": 500,\n",
      "    \"preprocessing\": False,\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"learning_rate_query\": 0.01,\n",
      "    \"epochs_query\": 20,\n",
      "    \"batch_size\": 32,\n",
      "    \"output_activation\": None,\n",
      "    \"contamination\": 0.1,\n",
      "    \"device\": None,\n",
      "    \"verbose\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary includes all parameters of the `__init__` method for the `AnoGAN` class, along with their default values. \n",
      "[Cache Hit] Using recent cache for DeepSVDD\n",
      "The `DeepSVDD` class in PyOD is designed for deep one-class classification, primarily used for anomaly detection. It trains a neural network to minimize the volume of a hypersphere that encloses the network representations of the data, effectively capturing the common factors of variation. This approach is detailed in the paper by Ruff et al. (2018).\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `__init__` method of the `DeepSVDD` class initializes the model with several parameters:\n",
      "\n",
      "- **n_features**: Number of features in the input data.\n",
      "- **c**: Deep SVDD center. If not provided, it is calculated based on the network's first forward pass.\n",
      "- **use_ae**: Boolean indicating whether to use the AutoEncoder type of DeepSVDD. Defaults to `False`.\n",
      "- **hidden_neurons**: List specifying the number of neurons per hidden layer. Defaults to `[64, 32]`.\n",
      "- **hidden_activation**: Activation function for hidden layers. Defaults to `'relu'`.\n",
      "- **output_activation**: Activation function for the output layer. Defaults to `'sigmoid'`.\n",
      "- **optimizer**: Optimizer for training. Defaults to `'adam'`.\n",
      "- **epochs**: Number of training epochs. Defaults to `100`.\n",
      "- **batch_size**: Number of samples per gradient update. Defaults to `32`.\n",
      "- **dropout_rate**: Dropout rate for layers. Defaults to `0.2`.\n",
      "- **l2_regularizer**: L2 regularization strength. Defaults to `0.1`.\n",
      "- **validation_size**: Proportion of data used for validation. Defaults to `0.1`.\n",
      "- **preprocessing**: Boolean indicating whether to standardize data. Defaults to `True`.\n",
      "- **verbose**: Verbosity mode. Defaults to `1`.\n",
      "- **random_state**: Seed for random number generation. Defaults to `None`.\n",
      "- **contamination**: Proportion of outliers in the data set. Defaults to `0.1`.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "After fitting the model, the following attributes are available:\n",
      "\n",
      "- **decision_scores_**: Anomaly scores of the training data. Higher scores indicate more abnormal instances.\n",
      "- **threshold_**: Threshold based on the contamination parameter, used to generate binary outlier labels.\n",
      "- **labels_**: Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Parameters Dictionary:**\n",
      "\n",
      "Here is a dictionary of all parameters for the `__init__` method, including their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"n_features\": None,\n",
      "    \"c\": None,\n",
      "    \"use_ae\": False,\n",
      "    \"hidden_neurons\": [64, 32],\n",
      "    \"hidden_activation\": \"relu\",\n",
      "    \"output_activation\": \"sigmoid\",\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"epochs\": 100,\n",
      "    \"batch_size\": 32,\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"l2_regularizer\": 0.1,\n",
      "    \"validation_size\": 0.1,\n",
      "    \"preprocessing\": True,\n",
      "    \"verbose\": 1,\n",
      "    \"random_state\": None,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Please note that `n_features` is a required parameter and does not have a default value. \n",
      "[Cache Hit] Using recent cache for ALAD\n",
      "The `ALAD` (Adversarially Learned Anomaly Detection) class in PyOD is designed for unsupervised anomaly detection using adversarial learning techniques. It implements the method described in the paper \"Adversarially Learned Anomaly Detection\" by Zenati et al. ([pyod.readthedocs.io](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `ALAD` class with several parameters that configure the model's architecture, training process, and other settings.\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `activation_hidden_gen` (str, optional, default='tanh'): Activation function for hidden layers in the generator (encoder and decoder) network.\n",
      "- `activation_hidden_disc` (str, optional, default='tanh'): Activation function for hidden layers in the discriminator networks.\n",
      "- `output_activation` (str, optional, default=None): Activation function for the output layers of the encoder and decoder.\n",
      "- `dropout_rate` (float, optional, default=0.2): Dropout rate applied across all layers.\n",
      "- `latent_dim` (int, optional, default=2): Dimensionality of the latent space.\n",
      "- `dec_layers` (list, optional, default=[5, 10, 25]): Number of nodes per hidden layer in the decoder network.\n",
      "- `enc_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the encoder network.\n",
      "- `disc_xx_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the discriminator network for input space.\n",
      "- `disc_zz_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the discriminator network for latent space.\n",
      "- `disc_xz_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the discriminator network for joint input and latent space.\n",
      "- `learning_rate_gen` (float, optional, default=0.0001): Learning rate for the generator network.\n",
      "- `learning_rate_disc` (float, optional, default=0.0001): Learning rate for the discriminator networks.\n",
      "- `add_recon_loss` (bool, optional, default=False): Whether to add reconstruction loss to the generator's loss function.\n",
      "- `lambda_recon_loss` (float, optional, default=0.1): Weight of the reconstruction loss in the generator's loss function.\n",
      "- `epochs` (int, optional, default=200): Number of training epochs.\n",
      "- `verbose` (int, optional, default=0): Verbosity mode.\n",
      "- `preprocessing` (bool, optional, default=False): Whether to apply data preprocessing (e.g., standardization).\n",
      "- `add_disc_zz_loss` (bool, optional, default=True): Whether to add loss from the latent space discriminator.\n",
      "- `spectral_normalization` (bool, optional, default=False): Whether to apply spectral normalization to the networks.\n",
      "- `batch_size` (int, optional, default=32): Number of samples per training batch.\n",
      "- `contamination` (float, optional, default=0.1): Proportion of outliers in the dataset.\n",
      "- `device` (torch.device, optional, default=None): Device to run the model on (e.g., 'cuda' or 'cpu').\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_` (numpy array of shape (n_samples,)): Outlier scores of the training data. Higher scores indicate more abnormal instances.\n",
      "- `threshold_` (float): Threshold based on the contamination parameter, used to generate binary outlier labels.\n",
      "- `labels_` (numpy array of shape (n_samples,)): Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"activation_hidden_gen\": \"tanh\",\n",
      "    \"activation_hidden_disc\": \"tanh\",\n",
      "    \"output_activation\": None,\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"latent_dim\": 2,\n",
      "    \"dec_layers\": [5, 10, 25],\n",
      "    \"enc_layers\": [25, 10, 5],\n",
      "    \"disc_xx_layers\": [25, 10, 5],\n",
      "    \"disc_zz_layers\": [25, 10, 5],\n",
      "    \"disc_xz_layers\": [25, 10, 5],\n",
      "    \"learning_rate_gen\": 0.0001,\n",
      "    \"learning_rate_disc\": 0.0001,\n",
      "    \"add_recon_loss\": False,\n",
      "    \"lambda_recon_loss\": 0.1,\n",
      "    \"epochs\": 200,\n",
      "    \"verbose\": 0,\n",
      "    \"preprocessing\": False,\n",
      "    \"add_disc_zz_loss\": True,\n",
      "    \"spectral_normalization\": False,\n",
      "    \"batch_size\": 32,\n",
      "    \"contamination\": 0.1,\n",
      "    \"device\": None\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary represents the parameters of the `ALAD` class's `__init__` method along with their default values. \n",
      "[Cache Hit] Using recent cache for AE1SVM\n",
      "The `AE1SVM` class in PyOD is an Autoencoder-based One-class Support Vector Machine designed for anomaly detection. It combines an autoencoder with a one-class SVM to effectively identify outliers in data.\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `AE1SVM` model with several parameters that control its architecture and training process.\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `hidden_neurons`: List of integers, optional (default=`[64, 32]`)\n",
      "  - Specifies the number of neurons in each hidden layer of the autoencoder.\n",
      "- `hidden_activation`: String, optional (default=`'relu'`)\n",
      "  - Defines the activation function for the hidden layers.\n",
      "- `batch_norm`: Boolean, optional (default=`True`)\n",
      "  - Indicates whether to apply batch normalization.\n",
      "- `learning_rate`: Float, optional (default=`1e-3`)\n",
      "  - Sets the learning rate for the optimizer.\n",
      "- `epochs`: Integer, optional (default=`50`)\n",
      "  - Determines the number of training epochs.\n",
      "- `batch_size`: Integer, optional (default=`32`)\n",
      "  - Specifies the size of each training batch.\n",
      "- `dropout_rate`: Float, optional (default=`0.2`)\n",
      "  - Sets the dropout rate for regularization.\n",
      "- `weight_decay`: Float, optional (default=`1e-5`)\n",
      "  - Defines the weight decay (L2 penalty) for the optimizer.\n",
      "- `preprocessing`: Boolean, optional (default=`True`)\n",
      "  - Indicates whether to apply standard scaling to the input data.\n",
      "- `loss_fn`: Callable, optional (default=`torch.nn.MSELoss()`)\n",
      "  - Specifies the loss function for reconstruction loss.\n",
      "- `contamination`: Float, optional (default=`0.1`)\n",
      "  - Represents the proportion of outliers in the data.\n",
      "- `alpha`: Float, optional (default=`1.0`)\n",
      "  - Weights the reconstruction loss in the final loss computation.\n",
      "- `sigma`: Float, optional (default=`1.0`)\n",
      "  - Sets the scaling factor for the random Fourier features.\n",
      "- `nu`: Float, optional (default=`0.1`)\n",
      "  - Parameter for the SVM loss.\n",
      "- `kernel_approx_features`: Integer, optional (default=`1000`)\n",
      "  - Number of random Fourier features to approximate the kernel.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_`:\n",
      "  - An array containing the outlier scores of the training data. Higher scores indicate a higher likelihood of being an outlier.\n",
      "- `threshold_`:\n",
      "  - The threshold value determined based on the `contamination` parameter, used to classify data points as inliers or outliers.\n",
      "- `labels_`:\n",
      "  - Binary labels for the training data, where 0 represents inliers and 1 represents outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"hidden_neurons\": [64, 32],\n",
      "    \"hidden_activation\": \"relu\",\n",
      "    \"batch_norm\": True,\n",
      "    \"learning_rate\": 1e-3,\n",
      "    \"epochs\": 50,\n",
      "    \"batch_size\": 32,\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"weight_decay\": 1e-5,\n",
      "    \"preprocessing\": True,\n",
      "    \"loss_fn\": \"torch.nn.MSELoss()\",\n",
      "    \"contamination\": 0.1,\n",
      "    \"alpha\": 1.0,\n",
      "    \"sigma\": 1.0,\n",
      "    \"nu\": 0.1,\n",
      "    \"kernel_approx_features\": 1000\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "*Note: The `loss_fn` parameter is represented as a string to ensure valid Python syntax for `ast.literal_eval`.*\n",
      "\n",
      "For more detailed information, you can refer to the official PyOD documentation:  \n",
      "[Cache Hit] Using recent cache for DevNet\n",
      "The `DevNet` class in PyOD is a deep learning-based anomaly detection model that utilizes deviation networks to identify outliers in data. It is implemented in the `pyod.models.devnet` module.\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method for the `DevNet` class is defined as follows:\n",
      "\n",
      "\n",
      "```python\n",
      "def __init__(self,\n",
      "             network_depth=2,\n",
      "             batch_size=512,\n",
      "             epochs=50,\n",
      "             nb_batch=20,\n",
      "             known_outliers=30,\n",
      "             cont_rate=0.02,\n",
      "             data_format=0,  # Assuming '0' for CSV\n",
      "             random_seed=42,\n",
      "             device=None,\n",
      "             contamination=0.1):\n",
      "    super(DevNet, self).__init__(contamination=contamination)\n",
      "    self._classes = 2\n",
      "    self.network_depth = network_depth\n",
      "    self.batch_size = batch_size\n",
      "    self.epochs = epochs\n",
      "    self.nb_batch = nb_batch\n",
      "    self.known_outliers = known_outliers\n",
      "    self.cont_rate = cont_rate\n",
      "    self.data_format = data_format\n",
      "    self.random_seed = random_seed\n",
      "    self.device = device\n",
      "    if self.device is None:\n",
      "        self.device = torch.device(\n",
      "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "```\n",
      "\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `network_depth` (default=2): Specifies the depth of the network.\n",
      "- `batch_size` (default=512): Defines the number of samples per batch during training.\n",
      "- `epochs` (default=50): Sets the number of training iterations over the entire dataset.\n",
      "- `nb_batch` (default=20): Indicates the number of batches per epoch.\n",
      "- `known_outliers` (default=30): Represents the number of known outliers used during training.\n",
      "- `cont_rate` (default=0.02): Denotes the contamination rate, i.e., the proportion of outliers in the dataset.\n",
      "- `data_format` (default=0): Assumes '0' for CSV format.\n",
      "- `random_seed` (default=42): Sets the seed for random number generation to ensure reproducibility.\n",
      "- `device` (default=None): Specifies the device for computation (e.g., 'cuda' or 'cpu'). If `None`, it defaults to 'cuda' if available; otherwise, 'cpu'.\n",
      "- `contamination` (default=0.1): Indicates the expected proportion of outliers in the dataset.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `self._classes`: Set to 2, indicating binary classification (inliers vs. outliers).\n",
      "- `self.network_depth`: Stores the specified network depth.\n",
      "- `self.batch_size`: Stores the batch size for training.\n",
      "- `self.epochs`: Stores the number of training epochs.\n",
      "- `self.nb_batch`: Stores the number of batches per epoch.\n",
      "- `self.known_outliers`: Stores the number of known outliers.\n",
      "- `self.cont_rate`: Stores the contamination rate.\n",
      "- `self.data_format`: Stores the data format.\n",
      "- `self.random_seed`: Stores the random seed value.\n",
      "- `self.device`: Stores the computation device.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"network_depth\": 2,\n",
      "    \"batch_size\": 512,\n",
      "    \"epochs\": 50,\n",
      "    \"nb_batch\": 20,\n",
      "    \"known_outliers\": 30,\n",
      "    \"cont_rate\": 0.02,\n",
      "    \"data_format\": 0,\n",
      "    \"random_seed\": 42,\n",
      "    \"device\": None,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary represents all parameters of the `__init__` method for the `DevNet` class, along with their default values. \n",
      "[Cache Hit] Using recent cache for LUNAR\n",
      "The `LUNAR` class in PyOD is designed for outlier detection by leveraging Graph Neural Networks to unify local outlier detection methods. It offers two model types: `SCORE_MODEL`, which directly outputs anomaly scores, and `WEIGHT_MODEL`, which outputs weights for k-nearest neighbor distances to compute anomaly scores. The class includes parameters for model configuration, negative sample generation, training settings, and data normalization.\n",
      "\n",
      "The `__init__` method of the `LUNAR` class has the following parameters with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"model_type\": \"WEIGHT\",\n",
      "    \"n_neighbours\": 5,\n",
      "    \"negative_sampling\": \"MIXED\",\n",
      "    \"val_size\": 0.1,\n",
      "    \"scaler\": \"MinMaxScaler()\",\n",
      "    \"epsilon\": 0.1,\n",
      "    \"proportion\": 1.0,\n",
      "    \"n_epochs\": 200,\n",
      "    \"lr\": 0.001,\n",
      "    \"wd\": 0.1,\n",
      "    \"verbose\": 0,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Note: The `scaler` parameter's default value is an instance of `MinMaxScaler()`, which is represented as a string to ensure valid Python syntax for `ast.literal_eval`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>infominer_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MO-GAAL</td>\n",
       "      <td>0.004609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SO-GAAL</td>\n",
       "      <td>0.000713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AutoEncoder</td>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VAE</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AnoGAN</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DeepSVDD</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ALAD</td>\n",
       "      <td>0.000404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AE1SVM</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DevNet</td>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LUNAR</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     algorithm  infominer_time\n",
       "0      MO-GAAL        0.004609\n",
       "1      SO-GAAL        0.000713\n",
       "2  AutoEncoder        0.000570\n",
       "3          VAE        0.000506\n",
       "4       AnoGAN        0.000483\n",
       "5     DeepSVDD        0.000439\n",
       "6         ALAD        0.000404\n",
       "7       AE1SVM        0.000387\n",
       "8       DevNet        0.000334\n",
       "9        LUNAR        0.000349"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mean    0.000879\n",
       "std     0.001315\n",
       "Name: infominer_time, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 2. Helper Function for PyOD InfoMiner\n",
    "\n",
    "def run_pyod_infominer(algorithms, train_path, test_path, params):\n",
    "    # Directly benchmark InfoMiner.query_docs without running the full pipeline\n",
    "    infom = InstrumentedInfoMiner()\n",
    "    results = []\n",
    "    for algo in algorithms:\n",
    "        # Time a single documentation query\n",
    "        _ = infom.query_docs(algo, None, 'pyod')\n",
    "        results.append({\n",
    "            'algorithm': algo,\n",
    "            'infominer_time': infom.last_query_duration\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Run Benchmark for Selected PyOD Algorithms\n",
    "algos = [\n",
    "    'MO-GAAL','SO-GAAL','AutoEncoder','VAE','AnoGAN',\n",
    "    'DeepSVDD','ALAD','AE1SVM','DevNet','LUNAR'\n",
    "]\n",
    "train_file = './data/glass_train.mat'\n",
    "test_file  = './data/glass_test.mat'\n",
    "params = {'contamination': 0.1}\n",
    "\n",
    "metrics = run_pyod_infominer(algos, train_file, test_file, params)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metrics)\n",
    "df.to_json('pyod_infominer_times.json', orient='records', indent=2)\n",
    "\n",
    "display(df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Summary of InfoMiner Time\n",
    "summary = df['infominer_time'].agg(['mean','std'])\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b2aff6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `AdONE` class in PyGOD is designed for adversarial outlier detection in attributed networks. It comprises both attribute and structure autoencoders, optimizing five distinct loss functions: attribute proximity, attribute homophily, structure proximity, structure homophily, and alignment losses. The model computes three outlier scores and averages them to derive an overall score. Notably, `AdONE` operates in a transductive manner, meaning it requires retraining when predicting on unseen data. ([docs.pygod.org](https://docs.pygod.org/en/stable/generated/pygod.detector.AdONE.html?utm_source=openai))\n",
      "\n",
      "**Initialization Parameters and Default Values:**\n",
      "\n",
      "The `__init__` method of the `AdONE` class accepts the following parameters:\n",
      "\n",
      "- `hid_dim` (int, optional): Hidden dimension of the model. Default: `64`.\n",
      "- `num_layers` (int, optional): Total number of layers in the model. Half (floor) are for the encoder, the other half (ceil) are for decoders. Default: `4`.\n",
      "- `dropout` (float, optional): Dropout rate. Default: `0.0`.\n",
      "- `weight_decay` (float, optional): Weight decay (L2 penalty). Default: `0.0`.\n",
      "- `act` (callable activation function or None, optional): Activation function if not None. Default: `torch.nn.functional.relu`.\n",
      "- `backbone` (torch.nn.Module): The backbone of AdONE is fixed to be MLP. Changing this parameter will not affect the model. Default: `None`.\n",
      "- `w1` (float, optional): Weight of structure proximity loss. Default: `0.2`.\n",
      "- `w2` (float, optional): Weight of structure homophily loss. Default: `0.2`.\n",
      "- `w3` (float, optional): Weight of attribute proximity loss. Default: `0.2`.\n",
      "- `w4` (float, optional): Weight of attribute homophily loss. Default: `0.2`.\n",
      "- `w5` (float, optional): Weight of alignment loss. Default: `0.2`.\n",
      "- `contamination` (float, optional): The proportion of outliers in the dataset, in the range (0., 0.5]. Used to define the threshold on the decision function. Default: `0.1`.\n",
      "- `lr` (float, optional): Learning rate. Default: `0.004`.\n",
      "- `epoch` (int, optional): Maximum number of training epochs. Default: `100`.\n",
      "- `gpu` (int): GPU index; `-1` for using CPU. Default: `-1`.\n",
      "- `batch_size` (int, optional): Minibatch size; `0` for full batch training. Default: `0`.\n",
      "- `num_neigh` (int, optional): Number of neighbors in sampling; `-1` for all neighbors. Default: `-1`.\n",
      "- `verbose` (int, optional): Verbosity mode, ranging from `0` to `3`. Larger values print more log information. Default: `0`.\n",
      "- `save_emb` (bool, optional): Whether to save the embedding. Default: `False`.\n",
      "- `compile_model` (bool, optional): Whether to compile the model with `torch_geometric.compile`. Default: `False`.\n",
      "- `**kwargs`: Other parameters for the backbone model.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "Upon fitting the model, the following attributes become available:\n",
      "\n",
      "- `decision_score_` (torch.Tensor): Outlier scores of the training data; higher scores indicate outliers.\n",
      "- `threshold_` (float): Threshold based on `contamination`, determining the most abnormal samples in `decision_score_`.\n",
      "- `label_` (torch.Tensor): Binary labels of the training data; `0` for inliers and `1` for outliers.\n",
      "- `emb` (torch.Tensor or tuple of torch.Tensor or None): Learned node hidden embeddings of shape \\(N \\times\\) `hid_dim`. Available when `save_emb` is `True`.\n",
      "- `attribute_score_` (torch.Tensor): Attribute outlier score.\n",
      "- `structural_score_` (torch.Tensor): Structural outlier score.\n",
      "- `combined_score_` (torch.Tensor): Combined outlier score.\n",
      "\n",
      "**Python Dictionary of `__init__` Parameters with Default Values:**\n",
      "\n",
      "Here is a Python dictionary representing the `__init__` parameters and their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"hid_dim\": 64,\n",
      "    \"num_layers\": 4,\n",
      "    \"dropout\": 0.0,\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"act\": \"torch.nn.functional.relu\",\n",
      "    \"backbone\": None,\n",
      "    \"w1\": 0.2,\n",
      "    \"w2\": 0.2,\n",
      "    \"w3\": 0.2,\n",
      "    \"w4\": 0.2,\n",
      "    \"w5\": 0.2,\n",
      "    \"contamination\": 0.1,\n",
      "    \"lr\": 0.004,\n",
      "    \"epoch\": 100,\n",
      "    \"gpu\": -1,\n",
      "    \"batch_size\": 0,\n",
      "    \"num_neigh\": -1,\n",
      "    \"verbose\": 0,\n",
      "    \"save_emb\": False,\n",
      "    \"compile_model\": False\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "*Note: The `act` parameter's default value is represented as a string to ensure valid Python syntax for `ast.literal_eval`.* \n",
      "[Cache Updated] Stored new documentation for AdONE\n",
      "The `ANOMALOUS` class in PyGOD is designed for anomaly detection on attributed networks. It implements a joint modeling approach that combines attribute information and network structure to identify anomalies.\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `ANOMALOUS` class is initialized with the following parameters:\n",
      "\n",
      "- `contamination` (float, optional): The proportion of outliers in the dataset, ranging from 0 to 0.5. Default is 0.1.\n",
      "\n",
      "- `verbose` (int, optional): Controls the verbosity of the output. Default is 0.\n",
      "\n",
      "- `save_emb` (bool, optional): Determines whether to save the embedding. Default is False.\n",
      "\n",
      "- `compile_model` (bool, optional): Indicates whether to compile the model with `torch_geometric.compile`. Default is False.\n",
      "\n",
      "- `**kwargs`: Additional parameters for the backbone.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "After fitting the model, the following attributes are available:\n",
      "\n",
      "- `decision_score_`: The outlier scores of the training data. Outliers tend to have higher scores.\n",
      "\n",
      "- `threshold_`: The threshold based on `contamination`, calculated for generating binary outlier labels.\n",
      "\n",
      "- `label_`: The binary labels of the training data, where 0 stands for inliers and 1 for outliers.\n",
      "\n",
      "- `emb`: The learned node hidden embeddings, available when `save_emb` is set to True.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "Here is a Python dictionary representing the parameters of the `__init__` method for the `ANOMALOUS` class, along with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"verbose\": 0,\n",
      "    \"save_emb\": False,\n",
      "    \"compile_model\": False,\n",
      "    \"**kwargs\": {}\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary includes all parameters with their default values, ensuring valid Python syntax for evaluation. \n",
      "[Cache Updated] Stored new documentation for ANOMALOUS\n",
      "The `AnomalyDAE` class in PyGOD is designed for anomaly detection on attributed networks. It employs a dual autoencoder architecture, consisting of both a structure autoencoder and an attribute autoencoder, to jointly learn node and attribute embeddings in a latent space. The structure autoencoder utilizes Graph Attention layers, and the reconstruction mean square errors from the decoders serve as structure and attribute anomaly scores, respectively. Additional penalties are applied to the reconstructed adjacency matrix and node attributes to enforce nonzero entries. ([pygod.readthedocs.io](https://pygod.readthedocs.io/en/latest/generated/pygod.detector.AnomalyDAE.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `AnomalyDAE` class is initialized with the following parameters:\n",
      "\n",
      "- `emb_dim` (int, optional): Embedding dimension of the model. Default: `64`.\n",
      "- `hid_dim` (int, optional): Hidden dimension of the model. Default: `64`.\n",
      "- `num_layers` (int, optional): Total number of layers of AnomalyDAE is fixed to be 4. Changing this parameter will not affect the model. Default: `4`.\n",
      "- `dropout` (float, optional): Dropout rate. Default: `0.0`.\n",
      "- `weight_decay` (float, optional): Weight decay (L2 penalty). Default: `0.0`.\n",
      "- `act` (callable activation function or None, optional): Activation function if not None. Default: `torch.nn.functional.relu`.\n",
      "- `backbone` (torch.nn.Module): The backbone of AnomalyDAE is fixed. Changing this parameter will not affect the model. Default: `None`.\n",
      "- `alpha` (float, optional): Weight between reconstruction of node feature and structure. Default: `0.5`.\n",
      "- `theta` (float, optional): The additional penalty for nonzero attribute. Default: `1.0`.\n",
      "- `eta` (float, optional): The additional penalty for nonzero structure. Default: `1.0`.\n",
      "- `contamination` (float, optional): The proportion of outliers in the dataset, in the range (0.0, 0.5]. Used when fitting to define the threshold on the decision function. Default: `0.1`.\n",
      "- `lr` (float, optional): Learning rate. Default: `0.004`.\n",
      "- `epoch` (int, optional): Maximum number of training epochs. Default: `5`.\n",
      "- `gpu` (int): GPU index, with `-1` indicating the use of CPU. Default: `-1`.\n",
      "- `batch_size` (int, optional): Minibatch size, with `0` indicating full batch training. Default: `0`.\n",
      "- `num_neigh` (int, optional): Number of neighbors in sampling, with `-1` indicating all neighbors. Default: `-1`.\n",
      "- `verbose` (int, optional): Verbosity mode, ranging from `0` to `3`, with larger values providing more log information. Default: `0`.\n",
      "- `save_emb` (bool, optional): Whether to save the embedding. Default: `False`.\n",
      "- `compile_model` (bool, optional): Whether to compile the model with `torch_geometric.compile`. Default: `False`.\n",
      "- `**kwargs`: Other parameters for the backbone model.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "Upon fitting the model, the following attributes are available:\n",
      "\n",
      "- `decision_score_` (torch.Tensor): The outlier scores of the training data, where outliers tend to have higher scores.\n",
      "- `threshold_` (float): The threshold based on `contamination`, calculated to generate binary outlier labels.\n",
      "- `label_` (torch.Tensor): The binary labels of the training data, with `0` indicating inliers and `1` indicating outliers.\n",
      "- `emb` (torch.Tensor or tuple of torch.Tensor or None): The learned node hidden embeddings of shape \\(N \\times\\) `hid_dim`. Available when `save_emb` is `True`. If the detector has not been fitted, `emb` is `None`. If the detector has multiple embeddings, `emb` is a tuple of torch.Tensor.\n",
      "\n",
      "**Python Dictionary of `__init__` Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"emb_dim\": 64,\n",
      "    \"hid_dim\": 64,\n",
      "    \"num_layers\": 4,\n",
      "    \"dropout\": 0.0,\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"act\": \"torch.nn.functional.relu\",\n",
      "    \"backbone\": None,\n",
      "    \"alpha\": 0.5,\n",
      "    \"theta\": 1.0,\n",
      "    \"eta\": 1.0,\n",
      "    \"contamination\": 0.1,\n",
      "    \"lr\": 0.004,\n",
      "    \"epoch\": 5,\n",
      "    \"gpu\": -1,\n",
      "    \"batch_size\": 0,\n",
      "    \"num_neigh\": -1,\n",
      "    \"verbose\": 0,\n",
      "    \"save_emb\": False,\n",
      "    \"compile_model\": False\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Note: The `act` parameter is represented as a string to ensure valid Python syntax for `ast.literal_eval`. \n",
      "[Cache Updated] Stored new documentation for AnomalyDAE\n",
      "The `CONAD` class in PyGOD is designed for Contrastive Attributed Network Anomaly Detection. It comprises a shared graph convolutional encoder, a structure reconstruction decoder, and an attribute reconstruction decoder. The model is trained using both contrastive loss and structure/attribute reconstruction loss, with the reconstruction mean square error of the decoders serving as structure and attribute anomaly scores, respectively. ([docs.pygod.org](https://docs.pygod.org/en/v0.4.0/generated/pygod.detector.CONAD.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `__init__` method of the `CONAD` class includes the following parameters:\n",
      "\n",
      "- `hid_dim` (int, optional): Hidden dimension of the model. Default: `64`.\n",
      "- `num_layers` (int, optional): Total number of layers in the model. Default: `4`.\n",
      "- `dropout` (float, optional): Dropout rate. Default: `0.0`.\n",
      "- `weight_decay` (float, optional): Weight decay (L2 penalty). Default: `0.0`.\n",
      "- `act` (callable activation function or None, optional): Activation function if not None. Default: `torch.nn.functional.relu`.\n",
      "- `sigmoid_s` (bool, optional): Whether to use the sigmoid function to scale the reconstructed structure. Default: `False`.\n",
      "- `backbone` (torch.nn.Module, optional): The backbone of the deep detector implemented in PyG. Default: `torch_geometric.nn.GCN`.\n",
      "- `contamination` (float, optional): The proportion of outliers in the dataset, in the range (0., 0.5]. Used to define the threshold on the decision function. Default: `0.1`.\n",
      "- `lr` (float, optional): Learning rate. Default: `0.004`.\n",
      "- `epoch` (int, optional): Maximum number of training epochs. Default: `100`.\n",
      "- `gpu` (int): GPU index, with `-1` indicating the use of CPU. Default: `-1`.\n",
      "- `batch_size` (int, optional): Minibatch size, with `0` indicating full batch training. Default: `0`.\n",
      "- `num_neigh` (int, optional): Number of neighbors in sampling, with `-1` indicating all neighbors. Default: `-1`.\n",
      "- `weight` (float, optional): Weight between reconstruction of node features and structure. Default: `0.5`.\n",
      "- `eta` (float, optional): Weight between contrastive and reconstruction losses. Default: `0.5`.\n",
      "- `margin` (float, optional): Margin in margin ranking loss. Default: `0.5`.\n",
      "- `r` (float, optional): The rate of augmented anomalies. Default: `0.2`.\n",
      "- `m` (int, optional): For densely connected nodes, the number of edges to add. Default: `50`.\n",
      "- `k` (int, optional): Same as `k` in `pygod.generator.gen_contextual_outlier`. Default: `50`.\n",
      "- `f` (int, optional): For disproportionate nodes, the scale factor applied to their attribute value. Default: `10`.\n",
      "- `verbose` (int, optional): Verbosity mode, ranging from `0` to `3`. Larger values result in more log information. Default: `0`.\n",
      "- `save_emb` (bool, optional): Whether to save the embedding. Default: `False`.\n",
      "- `compile_model` (bool, optional): Whether to compile the model with `torch_geometric.compile`. Default: `False`.\n",
      "- `**kwargs` (optional): Additional arguments for the backbone.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "After fitting the model, the following attributes are available:\n",
      "\n",
      "- `decision_score_` (torch.Tensor): The outlier scores of the training data, where outliers tend to have higher scores.\n",
      "- `threshold_` (float): The threshold based on `contamination`, calculated to generate binary outlier labels.\n",
      "- `label_` (torch.Tensor): The binary labels of the training data, with `0` indicating inliers and `1` indicating outliers.\n",
      "- `emb` (torch.Tensor or tuple of torch.Tensor or None): The learned node hidden embeddings of shape \\(N \\times\\) `hid_dim`. Available when `save_emb` is `True`.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "Here is a Python dictionary containing all parameters of the `__init__` method for the `CONAD` class, along with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"hid_dim\": 64,\n",
      "    \"num_layers\": 4,\n",
      "    \"dropout\": 0.0,\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"act\": \"torch.nn.functional.relu\",\n",
      "    \"sigmoid_s\": False,\n",
      "    \"backbone\": \"torch_geometric.nn.GCN\",\n",
      "    \"contamination\": 0.1,\n",
      "    \"lr\": 0.004,\n",
      "    \"epoch\": 100,\n",
      "    \"gpu\": -1,\n",
      "    \"batch_size\": 0,\n",
      "    \"num_neigh\": -1,\n",
      "    \"weight\": 0.5,\n",
      "    \"eta\": 0.5,\n",
      "    \"margin\": 0.5,\n",
      "    \"r\": 0.2,\n",
      "    \"m\": 50,\n",
      "    \"k\": 50,\n",
      "    \"f\": 10,\n",
      "    \"verbose\": 0,\n",
      "    \"save_emb\": False,\n",
      "    \"compile_model\": False\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Note: In this dictionary, default values that are objects or functions (e.g., `torch.nn.functional.relu` and `torch_geometric.nn.GCN`) are represented as strings to ensure valid Python syntax for `ast.literal_eval`. \n",
      "[Cache Updated] Stored new documentation for CONAD\n",
      "The `DOMINANT` class in PyGOD is designed for deep anomaly detection on attributed networks. It comprises a shared graph convolutional encoder, a structure reconstruction decoder, and an attribute reconstruction decoder. The reconstruction mean squared errors from these decoders serve as structure and attribute anomaly scores, respectively. ([docs.pygod.org](https://docs.pygod.org/en/latest/generated/pygod.detector.DOMINANT.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `DOMINANT` class is initialized with the following parameters:\n",
      "\n",
      "- `hid_dim` (int, optional): Hidden dimension of the model. Default: `64`.\n",
      "- `num_layers` (int, optional): Total number of layers in the model. Half are for the encoder, and the other half for the decoders. Default: `4`.\n",
      "- `dropout` (float, optional): Dropout rate. Default: `0.0`.\n",
      "- `weight_decay` (float, optional): Weight decay (L2 penalty). Default: `0.0`.\n",
      "- `act` (callable activation function or None, optional): Activation function if not None. Default: `torch.nn.functional.relu`.\n",
      "- `sigmoid_s` (bool, optional): Whether to use the sigmoid function to scale the reconstructed structure. Default: `False`.\n",
      "- `backbone` (torch.nn.Module, optional): The backbone of the deep detector implemented in PyG. Default: `torch_geometric.nn.GCN`.\n",
      "- `contamination` (float, optional): The proportion of outliers in the dataset, in the range (0., 0.5]. Used to define the threshold on the decision function. Default: `0.1`.\n",
      "- `lr` (float, optional): Learning rate. Default: `0.004`.\n",
      "- `epoch` (int, optional): Maximum number of training epochs. Default: `100`.\n",
      "- `gpu` (int): GPU index, with `-1` indicating the use of CPU. Default: `-1`.\n",
      "- `batch_size` (int, optional): Minibatch size, with `0` indicating full batch training. Default: `0`.\n",
      "- `num_neigh` (int, optional): Number of neighbors in sampling, with `-1` indicating all neighbors. Default: `-1`.\n",
      "- `weight` (float, optional): Weight between reconstruction of node features and structure. Default: `0.5`.\n",
      "- `verbose` (int, optional): Verbosity mode, ranging from `0` to `3`. Larger values result in more log information. Default: `0`.\n",
      "- `save_emb` (bool, optional): Whether to save the embedding. Default: `False`.\n",
      "- `compile_model` (bool, optional): Whether to compile the model with `torch_geometric.compile`. Default: `False`.\n",
      "- `**kwargs` (optional): Additional arguments for the backbone.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "After fitting the model, the following attributes are available:\n",
      "\n",
      "- `decision_score_` (torch.Tensor): Outlier scores of the training data, where higher scores indicate outliers.\n",
      "- `threshold_` (float): Threshold based on `contamination`, calculated as the top `N × contamination` most abnormal samples in `decision_score_`.\n",
      "- `label_` (torch.Tensor): Binary labels of the training data, with `0` for inliers and `1` for outliers, determined by applying `threshold_` on `decision_score_`.\n",
      "- `emb` (torch.Tensor or tuple of torch.Tensor or None): Learned node hidden embeddings of shape `N × hid_dim`. Available when `save_emb` is `True`. If the detector has multiple embeddings, `emb` is a tuple of torch.Tensor.\n",
      "\n",
      "**Parameters Dictionary:**\n",
      "\n",
      "Here is a Python dictionary containing all parameters of the `__init__` method for the `DOMINANT` class, along with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"hid_dim\": 64,\n",
      "    \"num_layers\": 4,\n",
      "    \"dropout\": 0.0,\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"act\": \"torch.nn.functional.relu\",\n",
      "    \"sigmoid_s\": False,\n",
      "    \"backbone\": \"torch_geometric.nn.GCN\",\n",
      "    \"contamination\": 0.1,\n",
      "    \"lr\": 0.004,\n",
      "    \"epoch\": 100,\n",
      "    \"gpu\": -1,\n",
      "    \"batch_size\": 0,\n",
      "    \"num_neigh\": -1,\n",
      "    \"weight\": 0.5,\n",
      "    \"verbose\": 0,\n",
      "    \"save_emb\": False,\n",
      "    \"compile_model\": False,\n",
      "    \"**kwargs\": {}\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Note: In this dictionary, default values that are objects or functions (e.g., `torch.nn.functional.relu` and `torch_geometric.nn.GCN`) are represented as strings to ensure valid Python syntax for `ast.literal_eval`. \n",
      "[Cache Updated] Stored new documentation for DOMINAT\n",
      "The `DONE` class in PyGOD is a deep learning model designed for outlier detection in attributed networks. It comprises both an attribute autoencoder and a structure autoencoder, optimizing five distinct losses: attribute proximity, attribute homophily, structure proximity, structure homophily, and a combination loss. The model computes three outlier scores and averages them to produce an overall score. Notably, `DONE` operates in a transductive manner, meaning it is intended for use with the same data it was trained on; applying `predict` to unseen data will retrain the detector from scratch. ([pygod.readthedocs.io](https://pygod.readthedocs.io/en/latest/generated/pygod.detector.DONE.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `__init__` method for the `DONE` class includes the following parameters:\n",
      "\n",
      "- `hid_dim` (int, optional): Hidden dimension of the model. Default: `64`.\n",
      "- `num_layers` (int, optional): Total number of layers in the model. Half (floor) are for the encoder, and the other half (ceil) are for the decoders. Default: `4`.\n",
      "- `dropout` (float, optional): Dropout rate. Default: `0.0`.\n",
      "- `weight_decay` (float, optional): Weight decay (L2 penalty). Default: `0.0`.\n",
      "- `act` (callable activation function or None, optional): Activation function if not None. Default: `torch.nn.functional.relu`.\n",
      "- `backbone` (torch.nn.Module): The backbone of `DONE` is fixed to be MLP. Changing this parameter will not affect the model. Default: `None`.\n",
      "- `w1` (float, optional): Weight of structure proximity loss. Default: `0.2`.\n",
      "- `w2` (float, optional): Weight of structure homophily loss. Default: `0.2`.\n",
      "- `w3` (float, optional): Weight of attribute proximity loss. Default: `0.2`.\n",
      "- `w4` (float, optional): Weight of attribute homophily loss. Default: `0.2`.\n",
      "- `w5` (float, optional): Weight of combination loss. Default: `0.2`.\n",
      "- `contamination` (float, optional): The proportion of outliers in the dataset, in the range (0., 0.5]. Used to define the threshold on the decision function. Default: `0.1`.\n",
      "- `lr` (float, optional): Learning rate. Default: `0.004`.\n",
      "- `epoch` (int, optional): Maximum number of training epochs. Default: `100`.\n",
      "- `gpu` (int): GPU index; `-1` for using CPU. Default: `-1`.\n",
      "- `batch_size` (int, optional): Minibatch size; `0` for full batch training. Default: `0`.\n",
      "- `num_neigh` (int, optional): Number of neighbors in sampling; `-1` for all neighbors. Default: `-1`.\n",
      "- `verbose` (int, optional): Verbosity mode, ranging from `0` to `3`. Larger values print more log information. Default: `0`.\n",
      "- `save_emb` (bool, optional): Whether to save the embedding. Default: `False`.\n",
      "- `compile_model` (bool, optional): Whether to compile the model with `torch_geometric.compile`. Default: `False`.\n",
      "- `**kwargs`: Other parameters for the backbone model.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "After fitting the model, the following attributes are available:\n",
      "\n",
      "- `decision_score_` (torch.Tensor): Outlier scores of the training data; higher scores indicate outliers.\n",
      "- `threshold_` (float): Threshold based on `contamination`, calculated to generate binary outlier labels.\n",
      "- `label_` (torch.Tensor): Binary labels of the training data; `0` for inliers and `1` for outliers.\n",
      "- `emb` (torch.Tensor or tuple of torch.Tensor or None): Learned node hidden embeddings of shape \\(N \\times\\) `hid_dim`. Available when `save_emb` is `True`.\n",
      "- `attribute_score_` (torch.Tensor): Attribute outlier score.\n",
      "- `structural_score_` (torch.Tensor): Structural outlier score.\n",
      "- `combined_score_` (torch.Tensor): Combined outlier score.\n",
      "\n",
      "**Parameters Dictionary:**\n",
      "\n",
      "Here is a Python dictionary containing all parameters of the `__init__` method for the `DONE` class, along with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"hid_dim\": 64,\n",
      "    \"num_layers\": 4,\n",
      "    \"dropout\": 0.0,\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"act\": \"torch.nn.functional.relu\",\n",
      "    \"backbone\": None,\n",
      "    \"w1\": 0.2,\n",
      "    \"w2\": 0.2,\n",
      "    \"w3\": 0.2,\n",
      "    \"w4\": 0.2,\n",
      "    \"w5\": 0.2,\n",
      "    \"contamination\": 0.1,\n",
      "    \"lr\": 0.004,\n",
      "    \"epoch\": 100,\n",
      "    \"gpu\": -1,\n",
      "    \"batch_size\": 0,\n",
      "    \"num_neigh\": -1,\n",
      "    \"verbose\": 0,\n",
      "    \"save_emb\": False,\n",
      "    \"compile_model\": False,\n",
      "    \"**kwargs\": {}\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Note: The `act` parameter's default value is a function (`torch.nn.functional.relu`), so it is wrapped in quotes to ensure valid Python syntax for `ast.literal_eval`. \n",
      "[Cache Updated] Stored new documentation for DONE\n",
      "The `GAAN` (Generative Adversarial Attributed Network Anomaly Detection) class in PyGOD is designed for detecting anomalies in graph data by leveraging a generative adversarial framework. This framework comprises a generator, an encoder, and a discriminator, and evaluates anomalies based on reconstruction errors and recognition confidence.\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `GAAN` class is initialized with the following parameters:\n",
      "\n",
      "- `noise_dim` (int, optional): Dimension of the Gaussian random noise. Default is `32`.\n",
      "- `latent_dim` (int, optional): Dimension of the latent space. Default is `32`.\n",
      "- `hid_dim1` (int, optional): Hidden dimension of the first MLP layer. Default is `32`.\n",
      "- `hid_dim2` (int, optional): Hidden dimension of the second MLP layer. Default is `64`.\n",
      "- `hid_dim3` (int, optional): Hidden dimension of the third MLP layer. Default is `128`.\n",
      "- `num_layers` (int, optional): Total number of layers in the model. Half (rounded up) are for the encoder, and the other half (rounded down) are for the decoder. Default is `3`.\n",
      "- `dropout` (float, optional): Dropout rate. Default is `0.0`.\n",
      "- `weight_decay` (float, optional): Weight decay (L2 penalty). Default is `0.0`.\n",
      "- `act` (callable activation function or None, optional): Activation function if not None. Default is `torch.nn.functional.relu`.\n",
      "- `alpha` (float, optional): Loss balance weight for attribute and structure. Default is `0.2`.\n",
      "- `contamination` (float, optional): Proportion of outliers in the dataset, used to define the threshold on the decision function. Valid range is (0., 0.5). Default is `0.05`.\n",
      "- `lr` (float, optional): Learning rate. Default is `0.005`.\n",
      "- `epoch` (int, optional): Maximum number of training epochs. Default is `10`.\n",
      "- `gpu` (int): GPU index; `-1` for using CPU. Default is `-1`.\n",
      "- `verbose` (bool): Verbosity mode; set to `True` to print log information. Default is `False`.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "Upon initialization, the `GAAN` class sets the following attributes:\n",
      "\n",
      "- `generator`: The generator module of the GAAN model.\n",
      "- `encoder`: The encoder module of the GAAN model.\n",
      "- `discriminator`: The discriminator module of the GAAN model.\n",
      "- `device`: The device on which the model is run (`'cuda:{gpu}'` if GPU is available and `gpu` is set; otherwise `'cpu'`).\n",
      "\n",
      "**Python Dictionary of `__init__` Parameters with Default Values:**\n",
      "\n",
      "Here is a Python dictionary representing the `__init__` parameters of the `GAAN` class along with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"noise_dim\": 32,\n",
      "    \"latent_dim\": 32,\n",
      "    \"hid_dim1\": 32,\n",
      "    \"hid_dim2\": 64,\n",
      "    \"hid_dim3\": 128,\n",
      "    \"num_layers\": 3,\n",
      "    \"dropout\": 0.0,\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"act\": \"torch.nn.functional.relu\",\n",
      "    \"alpha\": 0.2,\n",
      "    \"contamination\": 0.05,\n",
      "    \"lr\": 0.005,\n",
      "    \"epoch\": 10,\n",
      "    \"gpu\": -1,\n",
      "    \"verbose\": False\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "*Note: The `act` parameter, which is a function (`torch.nn.functional.relu`), is represented as a string to ensure valid Python syntax for `ast.literal_eval`.*\n",
      "\n",
      "For more detailed information, you can refer to the [PyGOD documentation on GAAN](https://docs.pygod.org/en/latest/pygod.detector.GAAN.html). \n",
      "[Cache Updated] Stored new documentation for GAAN\n",
      "Error in response for GUIDE\n",
      "Response(id='resp_68100747ef7081928d59e2327bc889660c02b7449f3da470', created_at=1745880903.0, error=None, incomplete_details=IncompleteDetails(reason='max_output_tokens'), instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseFunctionWebSearch(id='ws_68100748fd488192ae14540e773071320c02b7449f3da470', status='completed', type='web_search_call')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[WebSearchTool(type='web_search_preview', search_context_size='medium', user_location=UserLocation(type='approximate', city=None, country='US', region=None, timezone=None))], top_p=1.0, max_output_tokens=2024, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='incomplete', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=490, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=2024, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=2514), user=None, store=True)\n",
      "The `Radar` class in PyGOD is designed for anomaly detection in attributed networks using residual analysis. It is a transductive model, meaning it operates on the entire graph during training and cannot be directly applied to unseen data without retraining. ([pygod.readthedocs.io](https://pygod.readthedocs.io/en/latest/generated/pygod.detector.Radar.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `Radar` class is initialized with the following parameters:\n",
      "\n",
      "- `gamma` (float, optional):\n",
      "  - Weight of node features relative to structure.\n",
      "  - Default: `1.0`.\n",
      "\n",
      "- `weight_decay` (float, optional):\n",
      "  - Weight decay (L2 penalty).\n",
      "  - Default: `0.0`.\n",
      "\n",
      "- `lr` (float, optional):\n",
      "  - Learning rate.\n",
      "  - Default: `0.004`.\n",
      "\n",
      "- `epoch` (int, optional):\n",
      "  - Maximum number of training epochs.\n",
      "  - Default: `100`.\n",
      "\n",
      "- `gpu` (int):\n",
      "  - GPU index; `-1` indicates using CPU.\n",
      "  - Default: `-1`.\n",
      "\n",
      "- `contamination` (float, optional):\n",
      "  - Proportion of outliers in the dataset, valid in the range (0., 0.5).\n",
      "  - Used to define the threshold on the decision function.\n",
      "  - Default: `0.1`.\n",
      "\n",
      "- `verbose` (int, optional):\n",
      "  - Verbosity mode, ranging from 0 to 3.\n",
      "  - Larger values result in more log information.\n",
      "  - Default: `0`.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "Upon initialization, the `Radar` class has the following attributes:\n",
      "\n",
      "- `gamma`:\n",
      "  - Stores the weight of node features relative to structure.\n",
      "\n",
      "- `weight_decay`:\n",
      "  - Stores the weight decay (L2 penalty).\n",
      "\n",
      "- `lr`:\n",
      "  - Stores the learning rate.\n",
      "\n",
      "- `epoch`:\n",
      "  - Stores the maximum number of training epochs.\n",
      "\n",
      "- `device`:\n",
      "  - Determines the computation device (CPU or GPU) based on the `gpu` parameter.\n",
      "\n",
      "- `model`:\n",
      "  - Initialized as `None`; will hold the internal model after fitting.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "Here is a Python dictionary representing all parameters of the `__init__` method for the `Radar` class, along with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"gamma\": 1.0,\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"lr\": 0.004,\n",
      "    \"epoch\": 100,\n",
      "    \"gpu\": -1,\n",
      "    \"contamination\": 0.1,\n",
      "    \"verbose\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary can be used to initialize the `Radar` class with default parameters or modified as needed for specific use cases. \n",
      "[Cache Updated] Stored new documentation for Radar\n",
      "The `SCAN` class in PyGOD is a Structural Clustering Algorithm for Networks that clusters nodes based solely on graph structure, without considering node features. This model outputs detected clusters instead of the \"outliers\" described in the original paper. ([pygod.readthedocs.io](https://pygod.readthedocs.io/en/latest/generated/pygod.detector.SCAN.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `SCAN` detector with the following parameters:\n",
      "\n",
      "- `eps` (float, optional): Neighborhood threshold. Default: `0.5`.\n",
      "- `mu` (int, optional): Minimal size of clusters. Default: `2`.\n",
      "- `contamination` (float, optional): Proportion of outliers in the dataset, valid in the range (0., 0.5). Used to define the threshold on the decision function. Default: `0.1`.\n",
      "- `verbose` (int, optional): Verbosity mode, ranging from 0 to 3. Larger values print more log information. Default: `0`.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "After fitting the model, the following attributes are available:\n",
      "\n",
      "- `decision_score_` (torch.Tensor): Outlier scores of the training data. Higher scores indicate more abnormal nodes.\n",
      "- `threshold_` (float): Threshold based on `contamination`, calculated for generating binary outlier labels.\n",
      "- `label_` (torch.Tensor): Binary labels of the training data, where 0 stands for inliers and 1 for outliers.\n",
      "- `hub_score_` (torch.Tensor): Binary hub scores of each node.\n",
      "- `scatter_score_` (torch.Tensor): Binary scatter scores of each node, representing the \"outlier\" scores in the original paper.\n",
      "\n",
      "**Parameters Dictionary:**\n",
      "\n",
      "Here is a Python dictionary containing all parameters of the `__init__` method for the `SCAN` class, along with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"eps\": 0.5,\n",
      "    \"mu\": 2,\n",
      "    \"contamination\": 0.1,\n",
      "    \"verbose\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary can be used to initialize the `SCAN` detector with default parameters or modified as needed. \n",
      "[Cache Updated] Stored new documentation for SCAN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>infominer_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>AdONE</td>\n",
       "      <td>12.433644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>ANOMALOUS</td>\n",
       "      <td>8.005516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>AnomalyDAE</td>\n",
       "      <td>11.383099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>CONAD</td>\n",
       "      <td>11.859761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>DOMINAT</td>\n",
       "      <td>13.374995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>DONE</td>\n",
       "      <td>11.698351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>GAAN</td>\n",
       "      <td>11.251443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>GUIDE</td>\n",
       "      <td>49.877362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>Radar</td>\n",
       "      <td>12.371550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>inj_cora</td>\n",
       "      <td>SCAN</td>\n",
       "      <td>9.055996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset   algorithm  infominer_time\n",
       "0  inj_cora       AdONE       12.433644\n",
       "1  inj_cora   ANOMALOUS        8.005516\n",
       "2  inj_cora  AnomalyDAE       11.383099\n",
       "3  inj_cora       CONAD       11.859761\n",
       "4  inj_cora     DOMINAT       13.374995\n",
       "5  inj_cora        DONE       11.698351\n",
       "6  inj_cora        GAAN       11.251443\n",
       "7  inj_cora       GUIDE       49.877362\n",
       "8  inj_cora       Radar       12.371550\n",
       "9  inj_cora        SCAN        9.055996"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>algorithm</th>\n",
       "      <th>ANOMALOUS</th>\n",
       "      <th>AdONE</th>\n",
       "      <th>AnomalyDAE</th>\n",
       "      <th>CONAD</th>\n",
       "      <th>DOMINAT</th>\n",
       "      <th>DONE</th>\n",
       "      <th>GAAN</th>\n",
       "      <th>GUIDE</th>\n",
       "      <th>Radar</th>\n",
       "      <th>SCAN</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inj_cora</th>\n",
       "      <td>8.005516</td>\n",
       "      <td>12.433644</td>\n",
       "      <td>11.383099</td>\n",
       "      <td>11.859761</td>\n",
       "      <td>13.374995</td>\n",
       "      <td>11.698351</td>\n",
       "      <td>11.251443</td>\n",
       "      <td>49.877362</td>\n",
       "      <td>12.37155</td>\n",
       "      <td>9.055996</td>\n",
       "      <td>15.131172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "algorithm  ANOMALOUS      AdONE  AnomalyDAE      CONAD    DOMINAT       DONE  \\\n",
       "dataset                                                                        \n",
       "inj_cora    8.005516  12.433644   11.383099  11.859761  13.374995  11.698351   \n",
       "\n",
       "algorithm       GAAN      GUIDE     Radar      SCAN       mean  \n",
       "dataset                                                         \n",
       "inj_cora   11.251443  49.877362  12.37155  9.055996  15.131172  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from instrumentation import InstrumentedInfoMiner\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# 1) Helper for PyGOD InfoMiner (includes dataset column)\n",
    "def run_pygod_infominer(algorithms, dataset_name, package_name='pygod'):\n",
    "    infom = InstrumentedInfoMiner()\n",
    "    results = []\n",
    "    for algo in algorithms:\n",
    "        infom.last_query_duration = 0.0\n",
    "        _ = infom.query_docs(algo, vectorstore=None, package_name=package_name)\n",
    "        results.append({\n",
    "            'dataset':        dataset_name,\n",
    "            'algorithm':      algo,\n",
    "            'infominer_time': infom.last_query_duration\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 2) Specify models & dataset\n",
    "pygod_algos  = [\n",
    "    'AdONE','ANOMALOUS','AnomalyDAE','CONAD','DOMINAT',\n",
    "    'DONE','GAAN','GUIDE','Radar','SCAN'\n",
    "]\n",
    "dataset_name = 'inj_cora'\n",
    "\n",
    "# 3) Run the benchmark\n",
    "metrics = run_pygod_infominer(pygod_algos, dataset_name)\n",
    "\n",
    "# 4) Build DataFrame and pivot\n",
    "df = pd.DataFrame(metrics)\n",
    "pivot = df.pivot(index='dataset', columns='algorithm', values='infominer_time')\n",
    "\n",
    "pivot['mean'] = pivot.mean(axis=1)\n",
    "# 5) Display in Jupyter\n",
    "display(df)      # shows the raw list of (algorithm, time)\n",
    "display(pivot)   # shows the single‐row table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f573410",
   "metadata": {},
   "source": [
    "##Token Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "088c7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark_pyod_skip_selector.py\n",
    "\n",
    "import sys, os, asyncio, pandas as pd\n",
    "from config.config import Config\n",
    "\n",
    "# 1) Add project root\n",
    "os.environ[\"OPENAI_API_KEY\"] = Config.OPENAI_API_KEY\n",
    "\n",
    "# 2) Global counter and patches for ChatOpenAI._call\n",
    "import  langchain_openai\n",
    "\n",
    "# 3) Global token counter using the API's usage fields\n",
    "class TokenCounter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.prompt_tokens     = 0\n",
    "        self.completion_tokens = 0\n",
    "        self.total_tokens      = 0\n",
    "    def add_usage(self, usage):\n",
    "        self.prompt_tokens     += getattr(usage, \"prompt_tokens\", 0)\n",
    "        self.completion_tokens += getattr(usage, \"completion_tokens\", 0)\n",
    "        self.total_tokens      += getattr(usage, \"total_tokens\", 0)\n",
    "\n",
    "counter = TokenCounter()\n",
    "\n",
    "# 4) Monkey-patch LangChain ChatOpenAI._call to capture usage\n",
    "_orig_llm_call = langchain_openai.ChatOpenAI._call\n",
    "def _patched_llm_call(self, messages, **kwargs):\n",
    "    resp = _orig_llm_call(self, messages, **kwargs)\n",
    "    usage = getattr(resp, \"usage\", None)\n",
    "    if usage:\n",
    "        counter.add_usage(usage)\n",
    "    return resp\n",
    "langchain_openai.ChatOpenAI._call = _patched_llm_call\n",
    "\n",
    "# 5) Monkey-patch openai.chat.completions.create for direct OpenAI calls\n",
    "_orig_create = openai.chat.completions.create\n",
    "def _patched_create(*args, **kwargs):\n",
    "    resp = _orig_create(*args, **kwargs)\n",
    "    usage = getattr(resp, \"usage\", None)\n",
    "    if usage:\n",
    "        counter.add_usage(usage)\n",
    "    return resp\n",
    "openai.chat.completions.create = _patched_create\n",
    "\n",
    "# 6) Monkey-patch OpenAI.responses.create (used by AgentInfoMiner)\n",
    "_orig_openai_init = openai.OpenAI.__init__\n",
    "def _patched_openai_init(self, *args, **kwargs):\n",
    "    _orig_openai_init(self, *args, **kwargs)\n",
    "    if hasattr(self, \"responses\") and hasattr(self.responses, \"create\"):\n",
    "        orig_resp = self.responses.create\n",
    "        def _wrapped_responses_create(*a, **kw):\n",
    "            resp = orig_resp(*a, **kw)\n",
    "            usage = getattr(resp, \"usage\", None)\n",
    "            if usage:\n",
    "                counter.add_usage(usage)\n",
    "            return resp\n",
    "        self.responses.create = _wrapped_responses_create\n",
    "\n",
    "openai.OpenAI.__init__ = _patched_openai_init\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4) Import your agents\n",
    "from agents.agent_infominer import AgentInfoMiner\n",
    "from agents.agent_coder    import AgentCoder\n",
    "\n",
    "# 5) Configuration\n",
    "ALGOS      = [\n",
    "    'MO-GAAL','SO-GAAL','AutoEncoder','VAE','AnoGAN',\n",
    "    'DeepSVDD','ALAD','AE1SVM','DevNet','LUNAR'\n",
    "]\n",
    "TRAIN_PATH = './data/glass.mat'\n",
    "TEST_PATH  = './data/glass.mat'\n",
    "PARAMS     = {'contamination': 0.1}\n",
    "PKG        = 'pyod'\n",
    "VECTORSTORE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "77928f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cache Hit] Using recent cache for MO-GAAL\n",
      "The `MO_GAAL` class in PyOD is designed for Multi-Objective Generative Adversarial Active Learning, which generates potential outliers to help classifiers effectively distinguish between normal data and outliers. To prevent mode collapse, it employs multiple generators with different objectives.\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `MO_GAAL` class with the following parameters:\n",
      "\n",
      "- **contamination**: float in (0., 0.5), optional (default=0.1)\n",
      "  - The proportion of outliers in the dataset. Used to define the threshold on the decision function.\n",
      "\n",
      "- **k**: int, optional (default=10)\n",
      "  - The number of sub-generators.\n",
      "\n",
      "- **stop_epochs**: int, optional (default=20)\n",
      "  - The number of training epochs. The total number of epochs equals three times this value.\n",
      "\n",
      "- **lr_d**: float, optional (default=0.01)\n",
      "  - Learning rate of the discriminator.\n",
      "\n",
      "- **lr_g**: float, optional (default=0.0001)\n",
      "  - Learning rate of the generator.\n",
      "\n",
      "- **momentum**: float, optional (default=0.9)\n",
      "  - Momentum parameter for SGD.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- **decision_scores_**: numpy array of shape (n_samples,)\n",
      "  - Outlier scores of the training data. Higher scores indicate more abnormal data points.\n",
      "\n",
      "- **threshold_**: float\n",
      "  - Threshold based on `contamination`, marking the most abnormal samples in `decision_scores_`.\n",
      "\n",
      "- **labels_**: int, either 0 or 1\n",
      "  - Binary labels of the training data, where 0 denotes inliers and 1 denotes outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"k\": 10,\n",
      "    \"stop_epochs\": 20,\n",
      "    \"lr_d\": 0.01,\n",
      "    \"lr_g\": 0.0001,\n",
      "    \"momentum\": 0.9\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary represents all parameters of the `__init__` method for the `MO_GAAL` class, along with their default values. \n",
      "[Cache Hit] Using recent cache for SO-GAAL\n",
      "The `SO-GAAL` (Single-Objective Generative Adversarial Active Learning) model in PyOD is designed to generate potential outliers to assist classifiers in effectively distinguishing outliers from normal data. To prevent mode collapse, the network structure can be expanded to multiple generators with different objectives, known as MO-GAAL. ([pyod.readthedocs.io](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/so_gaal.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method for the `SO_GAAL` class is defined as follows:\n",
      "\n",
      "\n",
      "```python\n",
      "def __init__(self, stop_epochs=20, lr_d=0.01, lr_g=0.0001, momentum=0.9, contamination=0.1):\n",
      "    super(SO_GAAL, self).__init__(contamination=contamination)\n",
      "    self.stop_epochs = stop_epochs\n",
      "    self.lr_d = lr_d\n",
      "    self.lr_g = lr_g\n",
      "    self.momentum = momentum\n",
      "```\n",
      "\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `stop_epochs` (int, optional, default=20): The number of epochs for training. The total number of epochs equals three times this value.\n",
      "- `lr_d` (float, optional, default=0.01): Learning rate for the discriminator.\n",
      "- `lr_g` (float, optional, default=0.0001): Learning rate for the generator.\n",
      "- `momentum` (float, optional, default=0.9): Momentum parameter for SGD.\n",
      "- `contamination` (float in (0., 0.5), optional, default=0.1): The proportion of outliers in the dataset. Used to define the threshold on the decision function.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_` (numpy array of shape (n_samples,)): Outlier scores of the training data. Higher scores indicate more abnormal instances.\n",
      "- `threshold_` (float): Threshold based on the `contamination` parameter, used to generate binary outlier labels.\n",
      "- `labels_` (int, either 0 or 1): Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"stop_epochs\": 20,\n",
      "    \"lr_d\": 0.01,\n",
      "    \"lr_g\": 0.0001,\n",
      "    \"momentum\": 0.9,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      " \n",
      "[Cache Hit] Using recent cache for AutoEncoder\n",
      "The `AutoEncoder` class in PyOD is a neural network model designed for unsupervised outlier detection by learning data representations and identifying anomalies through reconstruction errors. It shares similarities with Principal Component Analysis (PCA) in detecting outliers.\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `AutoEncoder` class is initialized with the following parameters:\n",
      "\n",
      "- **contamination**: `float` in (0., 0.5), optional (default=0.1)\n",
      "  - Proportion of outliers in the dataset, used to define the threshold on the decision function.\n",
      "\n",
      "- **preprocessing**: `bool`, optional (default=True)\n",
      "  - Indicates whether to apply preprocessing before training.\n",
      "\n",
      "- **lr**: `float`, optional (default=1e-3)\n",
      "  - Initial learning rate for the optimizer.\n",
      "\n",
      "- **epoch_num**: `int`, optional (default=10)\n",
      "  - Number of training epochs.\n",
      "\n",
      "- **batch_size**: `int`, optional (default=32)\n",
      "  - Batch size for training.\n",
      "\n",
      "- **optimizer_name**: `str`, optional (default='adam')\n",
      "  - Name of the optimizer used for training.\n",
      "\n",
      "- **device**: `str`, optional (default=None)\n",
      "  - Device to use for the model; if `None`, it is determined automatically.\n",
      "\n",
      "- **random_state**: `int`, optional (default=42)\n",
      "  - Random seed for reproducibility.\n",
      "\n",
      "- **use_compile**: `bool`, optional (default=False)\n",
      "  - Whether to compile the model; applicable for PyTorch version >= 2.0.0 and Python < 3.12.\n",
      "\n",
      "- **compile_mode**: `str`, optional (default='default')\n",
      "  - Mode to compile the model; options include “default”, “reduce-overhead”, “max-autotune”, or “max-autotune-no-cudagraphs”.\n",
      "\n",
      "- **verbose**: `int`, optional (default=1)\n",
      "  - Verbosity mode: 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "\n",
      "- **optimizer_params**: `dict`, optional (default={'weight_decay': 1e-5})\n",
      "  - Additional parameters for the optimizer, e.g., `{'weight_decay': 1e-5}`.\n",
      "\n",
      "- **hidden_neuron_list**: `list`, optional (default=[64, 32])\n",
      "  - Number of neurons per hidden layer; the network structure is [feature_size, 64, 32, 32, 64, feature_size].\n",
      "\n",
      "- **hidden_activation_name**: `str`, optional (default='relu')\n",
      "  - Activation function used in hidden layers.\n",
      "\n",
      "- **batch_norm**: `bool`, optional (default=True)\n",
      "  - Whether to apply Batch Normalization.\n",
      "\n",
      "- **dropout_rate**: `float` in (0., 1), optional (default=0.2)\n",
      "  - Dropout rate applied across all layers.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- **model**: `torch.nn.Module`\n",
      "  - The underlying AutoEncoder model.\n",
      "\n",
      "- **optimizer**: `torch.optim`\n",
      "  - Optimizer used to train the model.\n",
      "\n",
      "- **criterion**: `torch.nn.modules`\n",
      "  - Loss function used during training.\n",
      "\n",
      "- **decision_scores_**: `numpy array` of shape (n_samples,)\n",
      "  - Outlier scores of the training data; higher scores indicate more abnormal instances.\n",
      "\n",
      "- **threshold_**: `float`\n",
      "  - Threshold based on `contamination` to determine outliers.\n",
      "\n",
      "- **labels_**: `int`, either 0 or 1\n",
      "  - Binary labels of the training data; 0 for inliers and 1 for outliers.\n",
      "\n",
      "**Python Dictionary of `__init__` Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"preprocessing\": True,\n",
      "    \"lr\": 1e-3,\n",
      "    \"epoch_num\": 10,\n",
      "    \"batch_size\": 32,\n",
      "    \"optimizer_name\": \"adam\",\n",
      "    \"device\": None,\n",
      "    \"random_state\": 42,\n",
      "    \"use_compile\": False,\n",
      "    \"compile_mode\": \"default\",\n",
      "    \"verbose\": 1,\n",
      "    \"optimizer_params\": {\"weight_decay\": 1e-5},\n",
      "    \"hidden_neuron_list\": [64, 32],\n",
      "    \"hidden_activation_name\": \"relu\",\n",
      "    \"batch_norm\": True,\n",
      "    \"dropout_rate\": 0.2\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary provides a comprehensive overview of the initialization parameters and their default values for the `AutoEncoder` class in PyOD. \n",
      "[Cache Hit] Using recent cache for VAE\n",
      "The `VAE` (Variational Autoencoder) class in PyOD is designed for unsupervised outlier detection by reconstructing input data and evaluating reconstruction errors. It combines an encoder that maps input data to a latent space and a decoder that reconstructs the data from this latent representation. The model's loss function includes both reconstruction loss and Kullback-Leibler (KL) divergence, with an optional weighting parameter `beta` to emphasize the KL loss, facilitating the implementation of β-VAE.\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `VAE` class is initialized with the following parameters:\n",
      "\n",
      "- **contamination**: (float, default=0.1)\n",
      "  - Proportion of outliers in the dataset.\n",
      "- **preprocessing**: (bool, default=True)\n",
      "  - Whether to apply preprocessing before training.\n",
      "- **lr**: (float, default=1e-3)\n",
      "  - Initial learning rate for the optimizer.\n",
      "- **epoch_num**: (int, default=30)\n",
      "  - Number of training epochs.\n",
      "- **batch_size**: (int, default=32)\n",
      "  - Batch size for training.\n",
      "- **optimizer_name**: (str, default='adam')\n",
      "  - Name of the optimizer used for training.\n",
      "- **device**: (str, default=None)\n",
      "  - Device to use for computation (e.g., 'cpu', 'cuda').\n",
      "- **random_state**: (int, default=42)\n",
      "  - Random seed for reproducibility.\n",
      "- **use_compile**: (bool, default=False)\n",
      "  - Whether to compile the model before training.\n",
      "- **compile_mode**: (str, default='default')\n",
      "  - Compilation mode for the model.\n",
      "- **verbose**: (int, default=1)\n",
      "  - Verbosity level of training output.\n",
      "- **optimizer_params**: (dict, default={'weight_decay': 1e-5})\n",
      "  - Additional parameters for the optimizer.\n",
      "- **beta**: (float, default=1.0)\n",
      "  - Weight of the KL divergence term in the loss function.\n",
      "- **capacity**: (float, default=0.0)\n",
      "  - Maximum capacity of the loss bottleneck.\n",
      "- **encoder_neuron_list**: (list, default=[128, 64, 32])\n",
      "  - List specifying the number of neurons in each encoder layer.\n",
      "- **decoder_neuron_list**: (list, default=[32, 64, 128])\n",
      "  - List specifying the number of neurons in each decoder layer.\n",
      "- **latent_dim**: (int, default=2)\n",
      "  - Dimensionality of the latent space.\n",
      "- **hidden_activation_name**: (str, default='relu')\n",
      "  - Activation function for hidden layers.\n",
      "- **output_activation_name**: (str, default='sigmoid')\n",
      "  - Activation function for the output layer.\n",
      "- **batch_norm**: (bool, default=False)\n",
      "  - Whether to apply batch normalization.\n",
      "- **dropout_rate**: (float, default=0.2)\n",
      "  - Dropout rate for regularization.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- **decision_scores_**: (numpy array of shape (n_samples,))\n",
      "  - Outlier scores of the training data; higher scores indicate more abnormal instances.\n",
      "- **threshold_**: (float)\n",
      "  - Threshold based on the contamination parameter, used to generate binary outlier labels.\n",
      "- **labels_**: (numpy array of shape (n_samples,))\n",
      "  - Binary labels for the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Python Dictionary of `__init__` Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"contamination\": 0.1,\n",
      "    \"preprocessing\": True,\n",
      "    \"lr\": 1e-3,\n",
      "    \"epoch_num\": 30,\n",
      "    \"batch_size\": 32,\n",
      "    \"optimizer_name\": \"adam\",\n",
      "    \"device\": None,\n",
      "    \"random_state\": 42,\n",
      "    \"use_compile\": False,\n",
      "    \"compile_mode\": \"default\",\n",
      "    \"verbose\": 1,\n",
      "    \"optimizer_params\": {\"weight_decay\": 1e-5},\n",
      "    \"beta\": 1.0,\n",
      "    \"capacity\": 0.0,\n",
      "    \"encoder_neuron_list\": [128, 64, 32],\n",
      "    \"decoder_neuron_list\": [32, 64, 128],\n",
      "    \"latent_dim\": 2,\n",
      "    \"hidden_activation_name\": \"relu\",\n",
      "    \"output_activation_name\": \"sigmoid\",\n",
      "    \"batch_norm\": False,\n",
      "    \"dropout_rate\": 0.2\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary provides a comprehensive overview of the initialization parameters and their default values for the `VAE` class in PyOD. \n",
      "[Cache Hit] Using recent cache for AnoGAN\n",
      "The `AnoGAN` class in PyOD is designed for anomaly detection using Generative Adversarial Networks (GANs). It is implemented in the `pyod.models.anogan` module. ([pyod.readthedocs.io](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `AnoGAN` class with several parameters that configure the model's architecture, training process, and other settings.\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `activation_hidden` (str, optional, default='tanh'): Activation function for hidden layers.\n",
      "- `dropout_rate` (float, optional, default=0.2): Dropout rate applied across all layers.\n",
      "- `latent_dim_G` (int, optional, default=2): Dimensionality of the generator's latent space.\n",
      "- `G_layers` (list, optional, default=[20, 10, 3, 10, 20]): Number of nodes per hidden layer in the generator.\n",
      "- `D_layers` (list, optional, default=[20, 10, 5]): Number of nodes per hidden layer in the discriminator.\n",
      "- `index_D_layer_for_recon_error` (int, optional, default=1): Index of the discriminator's hidden layer used for reconstruction error computation.\n",
      "- `epochs` (int, optional, default=500): Number of training epochs.\n",
      "- `preprocessing` (bool, optional, default=False): Whether to apply data standardization.\n",
      "- `learning_rate` (float, optional, default=0.001): Learning rate for training the network.\n",
      "- `learning_rate_query` (float, optional, default=0.01): Learning rate for backpropagation steps to approximate the query sample in the generator's latent space.\n",
      "- `epochs_query` (int, optional, default=20): Number of epochs for approximating the query sample in the generator's latent space.\n",
      "- `batch_size` (int, optional, default=32): Number of samples per gradient update.\n",
      "- `output_activation` (str, optional, default=None): Activation function for the output layer.\n",
      "- `contamination` (float, optional, default=0.1): Proportion of outliers in the dataset.\n",
      "- `device` (optional, default=None): Device to run the model on (e.g., 'cpu' or 'cuda').\n",
      "- `verbose` (int, optional, default=0): Verbosity mode (0 = silent, 1 = progress bar).\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_`: Numpy array of shape (n_samples,). Outlier scores of the training data.\n",
      "- `threshold_`: Float. Threshold based on `contamination` to generate binary outlier labels.\n",
      "- `labels_`: Int, either 0 or 1. Binary labels of the training data (0 for inliers, 1 for outliers).\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"activation_hidden\": \"tanh\",\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"latent_dim_G\": 2,\n",
      "    \"G_layers\": [20, 10, 3, 10, 20],\n",
      "    \"D_layers\": [20, 10, 5],\n",
      "    \"index_D_layer_for_recon_error\": 1,\n",
      "    \"epochs\": 500,\n",
      "    \"preprocessing\": False,\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"learning_rate_query\": 0.01,\n",
      "    \"epochs_query\": 20,\n",
      "    \"batch_size\": 32,\n",
      "    \"output_activation\": None,\n",
      "    \"contamination\": 0.1,\n",
      "    \"device\": None,\n",
      "    \"verbose\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary includes all parameters of the `__init__` method for the `AnoGAN` class, along with their default values. \n",
      "[Cache Hit] Using recent cache for DeepSVDD\n",
      "The `DeepSVDD` class in PyOD is designed for deep one-class classification, primarily used for anomaly detection. It trains a neural network to minimize the volume of a hypersphere that encloses the network representations of the data, effectively capturing the common factors of variation. This approach is detailed in the paper by Ruff et al. (2018).\n",
      "\n",
      "**Initialization Function and Parameters:**\n",
      "\n",
      "The `__init__` method of the `DeepSVDD` class initializes the model with several parameters:\n",
      "\n",
      "- **n_features**: Number of features in the input data.\n",
      "- **c**: Deep SVDD center. If not provided, it is calculated based on the network's first forward pass.\n",
      "- **use_ae**: Boolean indicating whether to use the AutoEncoder type of DeepSVDD. Defaults to `False`.\n",
      "- **hidden_neurons**: List specifying the number of neurons per hidden layer. Defaults to `[64, 32]`.\n",
      "- **hidden_activation**: Activation function for hidden layers. Defaults to `'relu'`.\n",
      "- **output_activation**: Activation function for the output layer. Defaults to `'sigmoid'`.\n",
      "- **optimizer**: Optimizer for training. Defaults to `'adam'`.\n",
      "- **epochs**: Number of training epochs. Defaults to `100`.\n",
      "- **batch_size**: Number of samples per gradient update. Defaults to `32`.\n",
      "- **dropout_rate**: Dropout rate for layers. Defaults to `0.2`.\n",
      "- **l2_regularizer**: L2 regularization strength. Defaults to `0.1`.\n",
      "- **validation_size**: Proportion of data used for validation. Defaults to `0.1`.\n",
      "- **preprocessing**: Boolean indicating whether to standardize data. Defaults to `True`.\n",
      "- **verbose**: Verbosity mode. Defaults to `1`.\n",
      "- **random_state**: Seed for random number generation. Defaults to `None`.\n",
      "- **contamination**: Proportion of outliers in the data set. Defaults to `0.1`.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "After fitting the model, the following attributes are available:\n",
      "\n",
      "- **decision_scores_**: Anomaly scores of the training data. Higher scores indicate more abnormal instances.\n",
      "- **threshold_**: Threshold based on the contamination parameter, used to generate binary outlier labels.\n",
      "- **labels_**: Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Parameters Dictionary:**\n",
      "\n",
      "Here is a dictionary of all parameters for the `__init__` method, including their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"n_features\": None,\n",
      "    \"c\": None,\n",
      "    \"use_ae\": False,\n",
      "    \"hidden_neurons\": [64, 32],\n",
      "    \"hidden_activation\": \"relu\",\n",
      "    \"output_activation\": \"sigmoid\",\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"epochs\": 100,\n",
      "    \"batch_size\": 32,\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"l2_regularizer\": 0.1,\n",
      "    \"validation_size\": 0.1,\n",
      "    \"preprocessing\": True,\n",
      "    \"verbose\": 1,\n",
      "    \"random_state\": None,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Please note that `n_features` is a required parameter and does not have a default value. \n",
      "[Cache Hit] Using recent cache for ALAD\n",
      "The `ALAD` (Adversarially Learned Anomaly Detection) class in PyOD is designed for unsupervised anomaly detection using adversarial learning techniques. It implements the method described in the paper \"Adversarially Learned Anomaly Detection\" by Zenati et al. ([pyod.readthedocs.io](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html?utm_source=openai))\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `ALAD` class with several parameters that configure the model's architecture, training process, and other settings.\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `activation_hidden_gen` (str, optional, default='tanh'): Activation function for hidden layers in the generator (encoder and decoder) network.\n",
      "- `activation_hidden_disc` (str, optional, default='tanh'): Activation function for hidden layers in the discriminator networks.\n",
      "- `output_activation` (str, optional, default=None): Activation function for the output layers of the encoder and decoder.\n",
      "- `dropout_rate` (float, optional, default=0.2): Dropout rate applied across all layers.\n",
      "- `latent_dim` (int, optional, default=2): Dimensionality of the latent space.\n",
      "- `dec_layers` (list, optional, default=[5, 10, 25]): Number of nodes per hidden layer in the decoder network.\n",
      "- `enc_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the encoder network.\n",
      "- `disc_xx_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the discriminator network for input space.\n",
      "- `disc_zz_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the discriminator network for latent space.\n",
      "- `disc_xz_layers` (list, optional, default=[25, 10, 5]): Number of nodes per hidden layer in the discriminator network for joint input and latent space.\n",
      "- `learning_rate_gen` (float, optional, default=0.0001): Learning rate for the generator network.\n",
      "- `learning_rate_disc` (float, optional, default=0.0001): Learning rate for the discriminator networks.\n",
      "- `add_recon_loss` (bool, optional, default=False): Whether to add reconstruction loss to the generator's loss function.\n",
      "- `lambda_recon_loss` (float, optional, default=0.1): Weight of the reconstruction loss in the generator's loss function.\n",
      "- `epochs` (int, optional, default=200): Number of training epochs.\n",
      "- `verbose` (int, optional, default=0): Verbosity mode.\n",
      "- `preprocessing` (bool, optional, default=False): Whether to apply data preprocessing (e.g., standardization).\n",
      "- `add_disc_zz_loss` (bool, optional, default=True): Whether to add loss from the latent space discriminator.\n",
      "- `spectral_normalization` (bool, optional, default=False): Whether to apply spectral normalization to the networks.\n",
      "- `batch_size` (int, optional, default=32): Number of samples per training batch.\n",
      "- `contamination` (float, optional, default=0.1): Proportion of outliers in the dataset.\n",
      "- `device` (torch.device, optional, default=None): Device to run the model on (e.g., 'cuda' or 'cpu').\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_` (numpy array of shape (n_samples,)): Outlier scores of the training data. Higher scores indicate more abnormal instances.\n",
      "- `threshold_` (float): Threshold based on the contamination parameter, used to generate binary outlier labels.\n",
      "- `labels_` (numpy array of shape (n_samples,)): Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"activation_hidden_gen\": \"tanh\",\n",
      "    \"activation_hidden_disc\": \"tanh\",\n",
      "    \"output_activation\": None,\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"latent_dim\": 2,\n",
      "    \"dec_layers\": [5, 10, 25],\n",
      "    \"enc_layers\": [25, 10, 5],\n",
      "    \"disc_xx_layers\": [25, 10, 5],\n",
      "    \"disc_zz_layers\": [25, 10, 5],\n",
      "    \"disc_xz_layers\": [25, 10, 5],\n",
      "    \"learning_rate_gen\": 0.0001,\n",
      "    \"learning_rate_disc\": 0.0001,\n",
      "    \"add_recon_loss\": False,\n",
      "    \"lambda_recon_loss\": 0.1,\n",
      "    \"epochs\": 200,\n",
      "    \"verbose\": 0,\n",
      "    \"preprocessing\": False,\n",
      "    \"add_disc_zz_loss\": True,\n",
      "    \"spectral_normalization\": False,\n",
      "    \"batch_size\": 32,\n",
      "    \"contamination\": 0.1,\n",
      "    \"device\": None\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary represents the parameters of the `ALAD` class's `__init__` method along with their default values. \n",
      "[Cache Hit] Using recent cache for AE1SVM\n",
      "The `AE1SVM` class in PyOD is an Autoencoder-based One-class Support Vector Machine designed for anomaly detection. It combines an autoencoder with a one-class SVM to effectively identify outliers in data.\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method initializes the `AE1SVM` model with several parameters that control its architecture and training process.\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `hidden_neurons`: List of integers, optional (default=`[64, 32]`)\n",
      "  - Specifies the number of neurons in each hidden layer of the autoencoder.\n",
      "- `hidden_activation`: String, optional (default=`'relu'`)\n",
      "  - Defines the activation function for the hidden layers.\n",
      "- `batch_norm`: Boolean, optional (default=`True`)\n",
      "  - Indicates whether to apply batch normalization.\n",
      "- `learning_rate`: Float, optional (default=`1e-3`)\n",
      "  - Sets the learning rate for the optimizer.\n",
      "- `epochs`: Integer, optional (default=`50`)\n",
      "  - Determines the number of training epochs.\n",
      "- `batch_size`: Integer, optional (default=`32`)\n",
      "  - Specifies the size of each training batch.\n",
      "- `dropout_rate`: Float, optional (default=`0.2`)\n",
      "  - Sets the dropout rate for regularization.\n",
      "- `weight_decay`: Float, optional (default=`1e-5`)\n",
      "  - Defines the weight decay (L2 penalty) for the optimizer.\n",
      "- `preprocessing`: Boolean, optional (default=`True`)\n",
      "  - Indicates whether to apply standard scaling to the input data.\n",
      "- `loss_fn`: Callable, optional (default=`torch.nn.MSELoss()`)\n",
      "  - Specifies the loss function for reconstruction loss.\n",
      "- `contamination`: Float, optional (default=`0.1`)\n",
      "  - Represents the proportion of outliers in the data.\n",
      "- `alpha`: Float, optional (default=`1.0`)\n",
      "  - Weights the reconstruction loss in the final loss computation.\n",
      "- `sigma`: Float, optional (default=`1.0`)\n",
      "  - Sets the scaling factor for the random Fourier features.\n",
      "- `nu`: Float, optional (default=`0.1`)\n",
      "  - Parameter for the SVM loss.\n",
      "- `kernel_approx_features`: Integer, optional (default=`1000`)\n",
      "  - Number of random Fourier features to approximate the kernel.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `decision_scores_`:\n",
      "  - An array containing the outlier scores of the training data. Higher scores indicate a higher likelihood of being an outlier.\n",
      "- `threshold_`:\n",
      "  - The threshold value determined based on the `contamination` parameter, used to classify data points as inliers or outliers.\n",
      "- `labels_`:\n",
      "  - Binary labels for the training data, where 0 represents inliers and 1 represents outliers.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"hidden_neurons\": [64, 32],\n",
      "    \"hidden_activation\": \"relu\",\n",
      "    \"batch_norm\": True,\n",
      "    \"learning_rate\": 1e-3,\n",
      "    \"epochs\": 50,\n",
      "    \"batch_size\": 32,\n",
      "    \"dropout_rate\": 0.2,\n",
      "    \"weight_decay\": 1e-5,\n",
      "    \"preprocessing\": True,\n",
      "    \"loss_fn\": \"torch.nn.MSELoss()\",\n",
      "    \"contamination\": 0.1,\n",
      "    \"alpha\": 1.0,\n",
      "    \"sigma\": 1.0,\n",
      "    \"nu\": 0.1,\n",
      "    \"kernel_approx_features\": 1000\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "*Note: The `loss_fn` parameter is represented as a string to ensure valid Python syntax for `ast.literal_eval`.*\n",
      "\n",
      "For more detailed information, you can refer to the official PyOD documentation:  \n",
      "[Cache Hit] Using recent cache for DevNet\n",
      "The `DevNet` class in PyOD is a deep learning-based anomaly detection model that utilizes deviation networks to identify outliers in data. It is implemented in the `pyod.models.devnet` module.\n",
      "\n",
      "**Initialization Function (`__init__`):**\n",
      "\n",
      "The `__init__` method for the `DevNet` class is defined as follows:\n",
      "\n",
      "\n",
      "```python\n",
      "def __init__(self,\n",
      "             network_depth=2,\n",
      "             batch_size=512,\n",
      "             epochs=50,\n",
      "             nb_batch=20,\n",
      "             known_outliers=30,\n",
      "             cont_rate=0.02,\n",
      "             data_format=0,  # Assuming '0' for CSV\n",
      "             random_seed=42,\n",
      "             device=None,\n",
      "             contamination=0.1):\n",
      "    super(DevNet, self).__init__(contamination=contamination)\n",
      "    self._classes = 2\n",
      "    self.network_depth = network_depth\n",
      "    self.batch_size = batch_size\n",
      "    self.epochs = epochs\n",
      "    self.nb_batch = nb_batch\n",
      "    self.known_outliers = known_outliers\n",
      "    self.cont_rate = cont_rate\n",
      "    self.data_format = data_format\n",
      "    self.random_seed = random_seed\n",
      "    self.device = device\n",
      "    if self.device is None:\n",
      "        self.device = torch.device(\n",
      "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "```\n",
      "\n",
      "\n",
      "**Parameters:**\n",
      "\n",
      "- `network_depth` (default=2): Specifies the depth of the network.\n",
      "- `batch_size` (default=512): Defines the number of samples per batch during training.\n",
      "- `epochs` (default=50): Sets the number of training iterations over the entire dataset.\n",
      "- `nb_batch` (default=20): Indicates the number of batches per epoch.\n",
      "- `known_outliers` (default=30): Represents the number of known outliers used during training.\n",
      "- `cont_rate` (default=0.02): Denotes the contamination rate, i.e., the proportion of outliers in the dataset.\n",
      "- `data_format` (default=0): Assumes '0' for CSV format.\n",
      "- `random_seed` (default=42): Sets the seed for random number generation to ensure reproducibility.\n",
      "- `device` (default=None): Specifies the device for computation (e.g., 'cuda' or 'cpu'). If `None`, it defaults to 'cuda' if available; otherwise, 'cpu'.\n",
      "- `contamination` (default=0.1): Indicates the expected proportion of outliers in the dataset.\n",
      "\n",
      "**Attributes:**\n",
      "\n",
      "- `self._classes`: Set to 2, indicating binary classification (inliers vs. outliers).\n",
      "- `self.network_depth`: Stores the specified network depth.\n",
      "- `self.batch_size`: Stores the batch size for training.\n",
      "- `self.epochs`: Stores the number of training epochs.\n",
      "- `self.nb_batch`: Stores the number of batches per epoch.\n",
      "- `self.known_outliers`: Stores the number of known outliers.\n",
      "- `self.cont_rate`: Stores the contamination rate.\n",
      "- `self.data_format`: Stores the data format.\n",
      "- `self.random_seed`: Stores the random seed value.\n",
      "- `self.device`: Stores the computation device.\n",
      "\n",
      "**Python Dictionary of Parameters with Default Values:**\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"network_depth\": 2,\n",
      "    \"batch_size\": 512,\n",
      "    \"epochs\": 50,\n",
      "    \"nb_batch\": 20,\n",
      "    \"known_outliers\": 30,\n",
      "    \"cont_rate\": 0.02,\n",
      "    \"data_format\": 0,\n",
      "    \"random_seed\": 42,\n",
      "    \"device\": None,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "This dictionary represents all parameters of the `__init__` method for the `DevNet` class, along with their default values. \n",
      "[Cache Hit] Using recent cache for LUNAR\n",
      "The `LUNAR` class in PyOD is designed for outlier detection by leveraging Graph Neural Networks to unify local outlier detection methods. It offers two model types: `SCORE_MODEL`, which directly outputs anomaly scores, and `WEIGHT_MODEL`, which outputs weights for k-nearest neighbor distances to compute anomaly scores. The class includes parameters for model configuration, negative sample generation, training settings, and data normalization.\n",
      "\n",
      "The `__init__` method of the `LUNAR` class has the following parameters with their default values:\n",
      "\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"model_type\": \"WEIGHT\",\n",
      "    \"n_neighbours\": 5,\n",
      "    \"negative_sampling\": \"MIXED\",\n",
      "    \"val_size\": 0.1,\n",
      "    \"scaler\": \"MinMaxScaler()\",\n",
      "    \"epsilon\": 0.1,\n",
      "    \"proportion\": 1.0,\n",
      "    \"n_epochs\": 200,\n",
      "    \"lr\": 0.001,\n",
      "    \"wd\": 0.1,\n",
      "    \"verbose\": 0,\n",
      "    \"contamination\": 0.1\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Note: The `scaler` parameter's default value is an instance of `MinMaxScaler()`, which is represented as a string to ensure valid Python syntax for `ast.literal_eval`. \n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for algo in ALGOS:\n",
    "    # InfoMiner\n",
    "    counter.reset()\n",
    "    inf = AgentInfoMiner()\n",
    "    doc = inf.query_docs(algo, VECTORSTORE, PKG)\n",
    "    info_in  = counter.prompt_tokens\n",
    "    info_out = counter.completion_tokens\n",
    "\n",
    "    # Coder\n",
    "    counter.reset()\n",
    "    coder = AgentCoder()\n",
    "    _ = coder.generate_code(\n",
    "        algorithm        = algo,\n",
    "        data_path_train  = TRAIN_PATH,\n",
    "        data_path_test   = TEST_PATH,\n",
    "        algorithm_doc    = doc,\n",
    "        input_parameters = PARAMS,\n",
    "        package_name     = PKG\n",
    "    )\n",
    "    code_in  = counter.prompt_tokens\n",
    "    code_out = counter.completion_tokens\n",
    "\n",
    "    rows.append({\n",
    "        'algorithm':  algo,\n",
    "        'info_in':    info_in,\n",
    "        'info_out':   info_out,\n",
    "        'code_in':    code_in,\n",
    "        'code_out':   code_out\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a7e470c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>info_in</th>\n",
       "      <th>info_out</th>\n",
       "      <th>code_in</th>\n",
       "      <th>code_out</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MO-GAAL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SO-GAAL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AutoEncoder</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnoGAN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSVDD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALAD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AE1SVM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DevNet</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LUNAR</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             info_in  info_out  code_in  code_out\n",
       "algorithm                                        \n",
       "MO-GAAL            0         0        0         0\n",
       "SO-GAAL            0         0        0         0\n",
       "AutoEncoder        0         0        0         0\n",
       "VAE                0         0        0         0\n",
       "AnoGAN             0         0        0         0\n",
       "DeepSVDD           0         0        0         0\n",
       "ALAD               0         0        0         0\n",
       "AE1SVM             0         0        0         0\n",
       "DevNet             0         0        0         0\n",
       "LUNAR              0         0        0         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows).set_index(\"algorithm\")\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
